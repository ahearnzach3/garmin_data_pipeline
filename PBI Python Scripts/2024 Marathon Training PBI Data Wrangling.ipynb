{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e100c9",
   "metadata": {},
   "source": [
    "# 2024 Marathon Training Data Transformation and Pre-Processing for PBI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a2c33",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show All Columns and Rows when viewing Dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d0998",
   "metadata": {},
   "source": [
    "## Running Workout Data\n",
    "This dataset solely consists of my running data. Moreover, each row represents 1 run and all of the metrics associated with that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0070fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "dataframe = pd.read_csv(\"Running_Data_20241111.csv\")\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31900997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset\n",
    "df = dataframe.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that consist entirely of null values\n",
    "print(df.shape)\n",
    "print(df.dropna(axis = 1, how='all').shape) # Compare shape after dropping null columns\n",
    "\n",
    "df_cleaned = df.dropna(axis = 1, how='all')\n",
    "\n",
    "# View Results\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add '_' into Column Headers\n",
    "df_cleaned.columns = df_cleaned.columns.str.replace(' ', '_')\n",
    "\n",
    "# Remove special characters from column names\n",
    "df_cleaned.columns = df_cleaned.columns.str.replace(r'[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "# View Results\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d9a5e",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Distance_Group' Column to group runs into mileage buckets\n",
    "bins = [0, 3, 5, 7, 10, 13, float('inf')]\n",
    "labels = ['0-3 miles', '3-5 miles', '5-7 miles', '7-10 miles', '10-13 miles', '13+ miles']\n",
    "\n",
    "# Use Distance Value to assign Distance Group\n",
    "df_cleaned = df_cleaned.assign(Distance_Group=pd.cut(df_cleaned['Distance'], bins=bins, labels=labels, right=False))\n",
    "\n",
    "# View Results\n",
    "df_cleaned[['Distance', 'Distance_Group']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d57a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'Distance_Group' directly after 'Distance'\n",
    "columns = df_cleaned.columns.to_list()\n",
    "\n",
    "# Get the index of the 'Distance' column\n",
    "distance_index = columns.index('Distance')\n",
    "\n",
    "# Insert 'Distance Group' right after 'Distance'\n",
    "columns.insert(distance_index + 1, columns.pop(columns.index('Distance_Group')))\n",
    "\n",
    "# Update df_cleaned with new Column Order\n",
    "df_cleaned = df_cleaned[columns]\n",
    "\n",
    "# View Results\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e64afb",
   "metadata": {},
   "source": [
    "### Map each distance group category to an integer for sorting in PBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da12659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the mapping\n",
    "distance_group_mapping = {\n",
    "    \"0-3 miles\": 1,\n",
    "    \"3-5 miles\": 2,\n",
    "    \"5-7 miles\": 3,\n",
    "    \"7-10 miles\": 4,\n",
    "    \"10-13 miles\": 5,\n",
    "    \"13+ miles\": 6\n",
    "}\n",
    "\n",
    "# Create the 'DistanceGroupId' column by mapping 'Distance_Group'\n",
    "df_cleaned['DistanceGroupId'] = df_cleaned['Distance_Group'].map(distance_group_mapping)\n",
    "\n",
    "# Move 'DistanceGroupId' directly after 'Distance_Group'\n",
    "columns = df_cleaned.columns.to_list()\n",
    "\n",
    "# Get the index of the 'Distance_Group' column\n",
    "Distance_Group_index = columns.index('Distance_Group')\n",
    "\n",
    "# Insert 'Distance Group' right after 'Distance_Group'\n",
    "columns.insert(Distance_Group_index + 1, columns.pop(columns.index('DistanceGroupId')))\n",
    "\n",
    "# Update df_cleaned with new Column Order\n",
    "df_cleaned = df_cleaned[columns]\n",
    "\n",
    "# View Results\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017406ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Favorite' Column as I have not been updating that field\n",
    "df_cleaned = df_cleaned.drop('Favorite', axis = 1)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5841cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' to datetime and set the time to 00:00:00\n",
    "df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date']).dt.normalize()\n",
    "\n",
    "# Confirm Datatype Conversion\n",
    "print(df_cleaned.dtypes)\n",
    "\n",
    "# View Results\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns\n",
    "cols_to_drop = ['Best_Lap_Time' # Usually only a few seconds as it takes me a second to stop my watch\n",
    "                 ,'Number_of_Laps' # Directly Correlated with Miles \n",
    "                ,'Avg_GAP'] # I don't care about this metric\n",
    "\n",
    "# View Results\n",
    "df_cleaned = df_cleaned.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Week of Year Field\n",
    "df_cleaned['Week_of_Year'] = df_cleaned['Date'].dt.isocalendar().week ## This starts the week on Monday and not Sunday\n",
    "\n",
    "# # Adjust the week to start on Sunday by subtracting the weekday number from the date\n",
    "# df_cleaned['Adjusted_Date'] = df_cleaned['Date'] - pd.to_timedelta(df_cleaned['Date'].dt.weekday + 1, unit='D')\n",
    "\n",
    "# # Calculate week of the year based on the adjusted date (starting from Sunday)\n",
    "# df_cleaned['Week_of_Year'] = df_cleaned['Adjusted_Date'].dt.isocalendar().week\n",
    "\n",
    "# # Drop 'Adjusted_Date'\n",
    "# df_cleaned = df_cleaned.drop('Adjusted_Date', axis=1)\n",
    "\n",
    "# Create Month Field\n",
    "df_cleaned['Month_Numeric'] = df_cleaned['Date'].dt.month\n",
    "\n",
    "# Create Month Field with abbreviated month names\n",
    "df_cleaned['Month'] = df_cleaned['Date'].dt.strftime('%b')\n",
    "\n",
    "# Create Year Field\n",
    "df_cleaned['Year'] = df_cleaned['Date'].dt.year\n",
    "\n",
    "# View New Fields and Data Types\n",
    "print(df_cleaned[['Date','Week_of_Year','Month','Year']].dtypes)\n",
    "df_cleaned[['Date','Week_of_Year','Month','Year']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7964197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'Week_of_year','Month','Year' directly after 'Date'\n",
    "columns = df_cleaned.columns.to_list()\n",
    "\n",
    "# Get the index of the 'Weekly_Cumulative_Mins' column\n",
    "distance_index = columns.index('Date')\n",
    "\n",
    "# Insert 'Month' right after 'Date'\n",
    "columns.insert(distance_index + 1, columns.pop(columns.index('Month_Numeric')))\n",
    "\n",
    "# Insert 'Month' right after 'Date'\n",
    "columns.insert(distance_index + 2, columns.pop(columns.index('Month')))\n",
    "\n",
    "# Insert 'Year' right after 'Month'\n",
    "columns.insert(distance_index + 3, columns.pop(columns.index('Year')))\n",
    "\n",
    "# Insert 'Week_of_Year' right after 'Year'\n",
    "columns.insert(distance_index + 4, columns.pop(columns.index('Week_of_Year')))\n",
    "\n",
    "# Update df_cleaned with new Column Order\n",
    "df_cleaned = df_cleaned[columns]\n",
    "\n",
    "# View Results\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Time Column to ensure consistent formatting\n",
    "df_cleaned['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad367daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop milliseconds and keep only minutes and seconds\n",
    "def drop_milliseconds(val):\n",
    "    if isinstance(val, str) and '.' in val:\n",
    "        # Keep only the minutes and seconds part, discard milliseconds\n",
    "        minutes, _ = val.split('.')  # Drop the milliseconds\n",
    "        return minutes  # Return only the minutes and seconds part\n",
    "    return val  # Return original value if not in the expected format\n",
    "\n",
    "# List of columns to clean\n",
    "cols = ['Time', 'Avg_Pace', 'Best_Pace', 'Moving_Time', 'Elapsed_Time']\n",
    "\n",
    "# Apply the function to the specified columns to drop milliseconds\n",
    "for col in cols:\n",
    "    df_cleaned[col] = df_cleaned[col].apply(drop_milliseconds)\n",
    "\n",
    "# View dataframe\n",
    "print(df_cleaned.dtypes)\n",
    "df_cleaned[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce069c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert 'mm:ss' to seconds\n",
    "def convert_mmss_to_seconds(val):\n",
    "    if isinstance(val, str) and ':' in val:\n",
    "        minutes, seconds = val.split(':')\n",
    "        return int(minutes) * 60 + int(seconds)\n",
    "    return val  # Return the value if it's not in the expected format\n",
    "\n",
    "# Check and convert 'Avg_Pace' column\n",
    "df_cleaned['Avg_Pace'] = df_cleaned['Avg_Pace'].apply(convert_mmss_to_seconds)\n",
    "df_cleaned['Avg_Pace'] = pd.to_timedelta(df_cleaned['Avg_Pace'], unit='s')\n",
    "\n",
    "# Check and convert 'Best_Pace' column\n",
    "df_cleaned['Best_Pace'] = df_cleaned['Best_Pace'].apply(convert_mmss_to_seconds)\n",
    "df_cleaned['Best_Pace'] = pd.to_timedelta(df_cleaned['Best_Pace'], unit='s')\n",
    "df_cleaned[['Avg_Pace','Best_Pace']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b4c9e4",
   "metadata": {},
   "source": [
    "### This needs to be a separate chunk because we are looking at hours as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e688345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize time format (convert hh:mm to 0:mm:ss)\n",
    "def standardize_time_format(val):\n",
    "    if isinstance(val, str):\n",
    "        if ':' in val:\n",
    "            parts = val.split(':')\n",
    "            if len(parts) == 2:  # hh:mm format (2 parts)\n",
    "                return f'0:{parts[0]}:{parts[1]}'  # Convert to 0:mm:ss format\n",
    "            elif len(parts) == 3:  # hh:mm:ss format (3 parts)\n",
    "                return val  # Already in the correct hh:mm:ss format\n",
    "    return val  # Return original value if not a string or if it's already valid\n",
    "\n",
    "# List of columns to apply the conversion to\n",
    "cols = ['Time', 'Moving_Time', 'Elapsed_Time']\n",
    "\n",
    "# Apply the function to the specified columns to standardize the time format\n",
    "for col in cols:\n",
    "    df_cleaned[col] = df_cleaned[col].apply(standardize_time_format)\n",
    "\n",
    "# Convert the columns to timedelta\n",
    "df_cleaned['Time'] = pd.to_timedelta(df_cleaned['Time'], errors='coerce')\n",
    "df_cleaned['Moving_Time'] = pd.to_timedelta(df_cleaned['Moving_Time'], errors='coerce')\n",
    "df_cleaned['Elapsed_Time'] = pd.to_timedelta(df_cleaned['Elapsed_Time'], errors='coerce')\n",
    "\n",
    "# Create the 'Idle_Time' field by subtracting 'Moving_Time' from 'Elapsed_Time'\n",
    "df_cleaned['Idle_Time'] = df_cleaned['Elapsed_Time'] - df_cleaned['Moving_Time']\n",
    "\n",
    "# Display the updated dataframe\n",
    "df_cleaned[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess Data types after conversion\n",
    "df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess Null Values\n",
    "null_counts = df_cleaned.isna().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Year, Week and calculate cumulative sum\n",
    "df_cleaned['Weekly_Cumulative_Mins'] = df_cleaned.groupby(['Year', 'Week_of_Year'])['Time'].cumsum()\n",
    "\n",
    "# Calculate Weekly_Mins_Prior_to_Run by shifting the cumulative sum by one row\n",
    "df_cleaned['Weekly_Mins_Prior_to_Run'] = df_cleaned.groupby(['Year', 'Week_of_Year'])['Weekly_Cumulative_Mins'].shift(1, fill_value=pd.Timedelta(0))\n",
    "\n",
    "# Convert Timedelta to minutes\n",
    "df_cleaned['Weekly_Mins_Prior_to_Run'] = df_cleaned['Weekly_Mins_Prior_to_Run'].dt.total_seconds() / 60\n",
    "\n",
    "# Round the 'Weekly_Mins_Prior_to_Run' to 2 decimal places\n",
    "df_cleaned['Weekly_Mins_Prior_to_Run'] = df_cleaned['Weekly_Mins_Prior_to_Run'].round(2)\n",
    "\n",
    "# Display the updated DataFrame (optional)\n",
    "print(df_cleaned[['Weekly_Mins_Prior_to_Run']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07fa055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Year, Month and calculate cumulative sum\n",
    "df_cleaned['Monthly_Cumulative_Mins'] = df_cleaned.groupby(['Year', 'Month'])['Time'].cumsum()\n",
    "\n",
    "# Calculate Monthly_Mins_Prior_to_Run by shifting the cumulative sum by one row\n",
    "df_cleaned['Monthly_Mins_Prior_to_Run'] = df_cleaned.groupby(['Year', 'Month'])['Monthly_Cumulative_Mins'].shift(1, fill_value=pd.Timedelta(0))\n",
    "\n",
    "# Convert Timedelta to minutes\n",
    "df_cleaned['Monthly_Mins_Prior_to_Run'] = df_cleaned['Monthly_Mins_Prior_to_Run'].dt.total_seconds() / 60\n",
    "\n",
    "# Round the 'Monthly_Mins_Prior_to_Run' to 2 decimal places\n",
    "df_cleaned['Monthly_Mins_Prior_to_Run'] = df_cleaned['Monthly_Mins_Prior_to_Run'].round(2)\n",
    "\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder Columns: 'Weekly_Cumulative_Mins','Weekly_Mins_Prior_to_Run','Monthly_Cumulative_Mins','Monthly_Mins_Prior_to_Run'\n",
    "columns = df_cleaned.columns.to_list()\n",
    "\n",
    "# Get the index of the 'Time' column\n",
    "distance_index = columns.index('Time')\n",
    "\n",
    "# Insert 'Weekly_Cumulative_Mins' right after 'Time'\n",
    "columns.insert(distance_index + 1, columns.pop(columns.index('Weekly_Cumulative_Mins')))\n",
    "\n",
    "# Insert 'Weekly_Mins_Prior_to_Run' right after 'Weekly_Cumulative_Mins'\n",
    "columns.insert(distance_index + 2, columns.pop(columns.index('Weekly_Mins_Prior_to_Run')))\n",
    "\n",
    "# Insert 'Monthly_Cumulative_Mins' right after 'Weekly_Mins_Prior_to_Run'\n",
    "columns.insert(distance_index + 3, columns.pop(columns.index('Monthly_Cumulative_Mins')))\n",
    "\n",
    "# Insert 'Monthly_Mins_Prior_to_Run' right after 'Monthly_Cumulative_Mins'\n",
    "columns.insert(distance_index + 4, columns.pop(columns.index('Monthly_Mins_Prior_to_Run')))\n",
    "\n",
    "# Update df_cleaned with new Column Order\n",
    "df_cleaned = df_cleaned[columns]\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf847f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique value count across all columns\n",
    "unique_counts = df_cleaned.nunique()\n",
    "\n",
    "# Display the unique value counts for each column\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b807b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop columns with only one unique value\n",
    "def drop_single_value_columns(df):\n",
    "    # Identify columns with only one unique value\n",
    "    cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    \n",
    "    # Drop those columns\n",
    "    df_cleaned = df.drop(cols_to_drop, axis=1)\n",
    "    return df_cleaned\n",
    "\n",
    "# Example usage\n",
    "df_cleaned = drop_single_value_columns(df_cleaned)\n",
    "\n",
    "# Display the dataframe after dropping the columns\n",
    "df_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b11cf",
   "metadata": {},
   "source": [
    "### Conversion of Time-oriented fields to Duration in PBI was not working as expected\n",
    "Drop '0 Days' for Time-oriented fields and format as strings for PBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aed327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert timedelta to string and clean '0 days ' part\n",
    "def clean_timedelta_to_string(val):\n",
    "    if isinstance(val, pd.Timedelta):\n",
    "        # Convert timedelta to string and remove '0 days ' part\n",
    "        clean_value = str(val).split(' ')[-1]  # Keep only the 'hh:mm:ss' part\n",
    "        return clean_value  # Return the string in hh:mm:ss format\n",
    "    return val  # Return the original value if not a timedelta\n",
    "\n",
    "# Create a new DataFrame df_pbi by copying df_cleaned\n",
    "df_pbi = df_cleaned.copy()\n",
    "\n",
    "# List of columns to clean\n",
    "cols = ['Time', 'Avg_Pace', 'Weekly_Cumulative_Mins', 'Monthly_Cumulative_Mins', 'Best_Pace', 'Moving_Time', 'Elapsed_Time', 'Idle_Time']\n",
    "\n",
    "# Apply the function to the specified columns in df_pbi to remove '0 days'\n",
    "for col in cols:\n",
    "    df_pbi[col] = df_pbi[col].apply(clean_timedelta_to_string)\n",
    "\n",
    "# Check the data types after cleaning to ensure they are 'object' (string)\n",
    "print(df_pbi.dtypes)\n",
    "\n",
    "# View Results\n",
    "df_pbi.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pbi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71ba90",
   "metadata": {},
   "source": [
    "### Save Final Dataframe to a CSV for PBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_pbi.to_csv('PBI Data/Running_Data_Cleaned_PBI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282c0f8",
   "metadata": {},
   "source": [
    "### Create new dataframe for PBI that is filtered to 18 Week training Plan Dates Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0848c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Copy of Dataframe\n",
    "training_plan = df_pbi.copy()\n",
    "\n",
    "# Define the training start date (August 13, 2024)\n",
    "training_start_date = pd.to_datetime('2024-08-13')\n",
    "\n",
    "# Create the IsTrainingPlan column: 1 if the date is greater than or equal to training_start_date, otherwise 0\n",
    "training_plan['IsTrainingPlan'] = (training_plan['Date'] >= training_start_date).astype(int)\n",
    "\n",
    "# View Dataframe\n",
    "training_plan[['Date','IsTrainingPlan']]\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'IsTrainingPlan' = 1\n",
    "training_plan_filtered = training_plan[training_plan['IsTrainingPlan'] == 1]\n",
    "training_plan_filtered\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "training_plan_filtered.to_csv('PBI Data/Training_Plan_PBI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de4f15",
   "metadata": {},
   "source": [
    "## Sleep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca630c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in Sleep Data\n",
    "sleep_1_main = pd.read_json('Raw Data/Sleep_Data/2023-10-15_2024-01-23_117832404_sleepData.json')\n",
    "sleep_2_main = pd.read_json('Raw Data/Sleep_Data/2024-01-23_2024-05-02_117832404_sleepData.json')\n",
    "sleep_3_main = pd.read_json('Raw Data/Sleep_Data/2024-05-02_2024-08-10_117832404_sleepData.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "sleep_1 = sleep_1_main.copy()\n",
    "sleep_2 = sleep_2_main.copy()\n",
    "sleep_3 = sleep_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "sleep_1.head()\n",
    "\n",
    "# Assess sleep_1 characteristcs\n",
    "## print(sleep_1.shape)\n",
    "## print(sleep_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f1c94",
   "metadata": {},
   "source": [
    "### Combine 3 Sleep Data Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355b32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_sleep = pd.concat([sleep_1, sleep_2, sleep_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_sleep['calendarDate'] = pd.to_datetime(combined_sleep['calendarDate'])\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'sleepScores' column into separate columns\n",
    "combined_sleep = combined_sleep.join(combined_sleep['sleepScores'].apply(pd.Series))\n",
    "\n",
    "# Drop 'sleepScores' now that I have extracted information into other columns\n",
    "combined_sleep = combined_sleep.drop('sleepScores',axis=1)\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953bcbb",
   "metadata": {},
   "source": [
    "### Assess Null Values for Sleep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sleep start and end timestamps to datetime format\n",
    "combined_sleep['sleepStartTimestampGMT'] = pd.to_datetime(combined_sleep['sleepStartTimestampGMT'])\n",
    "combined_sleep['sleepEndTimestampGMT'] = pd.to_datetime(combined_sleep['sleepEndTimestampGMT'])\n",
    "\n",
    "# Calculate the time difference between sleepStart and sleepEnd\n",
    "combined_sleep['sleepDuration'] = combined_sleep['sleepEndTimestampGMT'] - combined_sleep['sleepStartTimestampGMT']\n",
    "\n",
    "# Create a new column where sleepDuration is in float hours\n",
    "combined_sleep['sleepDurationHours'] = (combined_sleep['sleepDuration'].dt.total_seconds() / 3600).round(1)  # Convert to hours as a float\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e42424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reorder Columns: \n",
    "columns = combined_sleep.columns.to_list()\n",
    "\n",
    "# Get the index of the 'calendarDate' column\n",
    "distance_index = columns.index('calendarDate')\n",
    "\n",
    "# Remove 'sleepDurationHours' and 'sleepDuration' columns from the list\n",
    "columns.remove('sleepDurationHours')\n",
    "columns.remove('sleepDuration')\n",
    "\n",
    "# Insert 'sleepDurationHours' right after 'calendarDate'\n",
    "columns.insert(distance_index + 1, 'sleepDurationHours')\n",
    "\n",
    "# Insert 'sleepDuration' right after 'sleepDurationHours'\n",
    "columns.insert(distance_index + 2, 'sleepDuration')\n",
    "\n",
    "# Reassign the new column order to the DataFrame\n",
    "combined_sleep = combined_sleep[columns]\n",
    "\n",
    "# Drop Columns\n",
    "cols_to_drop = ['sleepStartTimestampGMT','sleepEndTimestampGMT','sleepWindowConfirmationType']\n",
    "combined_sleep = combined_sleep.drop(cols_to_drop,axis=1)\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the last column by position using iloc\n",
    "combined_sleep = combined_sleep.iloc[:, :-1]\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15323c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Rows in df where 'remSleepSeconds' is null\n",
    "combined_sleep[combined_sleep['remSleepSeconds'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to replace all null values with the column's average value for float64 datatype columns\n",
    "def fill_null_with_mean(df, columns):\n",
    "    for col in columns:\n",
    "        mean_value = df[col].mean()\n",
    "        df[col] = df[col].fillna(mean_value)  # Assign the filled column back to the DataFrame\n",
    "    return df\n",
    "\n",
    "# Get the list of float64 columns\n",
    "float_columns = [col for col in combined_sleep.columns if combined_sleep[col].dtype == 'float64']\n",
    "\n",
    "# Apply the function to replace null values with the mean\n",
    "combined_sleep = fill_null_with_mean(combined_sleep, float_columns)\n",
    "\n",
    "# View the DataFrame\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca122e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rows where 'calendarDate' isna()\n",
    "## combined_sleep[combined_sleep['calendarDate'].isna()]\n",
    "\n",
    "# Manually assign the correct dates to the specific indices where 'calendarDate' is NaT\n",
    "combined_sleep.loc[109, 'calendarDate'] = pd.Timestamp('2024-03-14')\n",
    "combined_sleep.loc[257, 'calendarDate'] = pd.Timestamp('2024-08-09')\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34666f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Make sure 'sleepDurationHours' does not have NaN values before applying\n",
    "combined_sleep['sleepDuration'] = combined_sleep.apply(\n",
    "    lambda row: pd.Timedelta(hours=row['sleepDurationHours']) if pd.isna(row['sleepDuration']) and pd.notna(row['sleepDurationHours']) else row['sleepDuration'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5894f74",
   "metadata": {},
   "source": [
    "#### Re-format 'sleepDuration' so that the field can be converted to a duration dtype in PBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab122aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timedelta to string and remove '0 days '\n",
    "combined_sleep['sleepDurationFormatted'] = combined_sleep['sleepDuration'].apply(lambda x: str(x).split(' ')[-1])\n",
    "\n",
    "# Check the result\n",
    "print(combined_sleep[['sleepDuration', 'sleepDurationFormatted']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e081ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder Columns\n",
    "columns = combined_sleep.columns.to_list()\n",
    "\n",
    "# Get the index of the 'sleepDuration' column\n",
    "sleep_duration_index = columns.index('sleepDuration')\n",
    "\n",
    "# Remove 'sleepDurationFormatted' column from the list\n",
    "columns.remove('sleepDurationFormatted')\n",
    "\n",
    "# Insert 'sleepDurationFormatted' right after 'sleepDuration'\n",
    "columns.insert(sleep_duration_index + 1, 'sleepDurationFormatted')\n",
    "\n",
    "# Reassign the new column order to the DataFrame\n",
    "combined_sleep = combined_sleep[columns]\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e30883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round sleepDuration to seconds to remove microseconds and nanoseconds\n",
    "combined_sleep['sleepDuration'] = combined_sleep['sleepDuration'].dt.round('s')  # Use 's' instead of 'S'\n",
    "\n",
    "# If you want to format the duration as 'hh:mm:ss' without nanoseconds\n",
    "combined_sleep['sleepDurationFormatted'] = combined_sleep['sleepDuration'].apply(lambda x: str(x).split(' ')[-1])\n",
    "\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52454a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace NaN values in 'insight' with \"NONE\"\n",
    "combined_sleep['insight'] = combined_sleep['insight'].fillna(\"NONE\")\n",
    "\n",
    "# View value counts for 'feedback' categories\n",
    "## combined_sleep['feedback'].value_counts()\n",
    "\n",
    "# Replace NaN values in 'insight' with \"NONE\"\n",
    "combined_sleep['feedback'] = combined_sleep['feedback'].fillna(\"NONE\")\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9a8a8",
   "metadata": {},
   "source": [
    "### Convert columns LIKE '%Seconds%' to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6a0d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_sleep_cleaned = combined_sleep.copy()\n",
    "\n",
    "# Convert seconds to hours (1 hour = 3600 seconds)\n",
    "def seconds_to_hours(seconds):\n",
    "    return round(seconds / 3600, 1)\n",
    "\n",
    "# Identify columns that contain 'Seconds' in their name\n",
    "columns_to_convert = [col for col in combined_sleep_cleaned.columns if 'Seconds' in col]\n",
    "\n",
    "# Apply the conversion function to these columns\n",
    "for col in columns_to_convert:\n",
    "    combined_sleep_cleaned[col] = combined_sleep_cleaned[col].apply(seconds_to_hours)\n",
    "\n",
    "combined_sleep_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514176f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_sleep_cleaned[combined_sleep_cleaned['sleepDuration'].isna()] ## None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b788970",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86461b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_seconds_to_hours(df):\n",
    "    # Rename columns by replacing 'Seconds' with 'Hours'\n",
    "    df = df.rename(columns={col: col.replace('Seconds', 'Hours') for col in df.columns if 'Seconds' in col})\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "combined_sleep_cleaned = rename_seconds_to_hours(combined_sleep_cleaned)\n",
    "\n",
    "combined_sleep_cleaned.head()\n",
    "\n",
    "# Confirm that there is 1 row per CalendarDate\n",
    "## combined_sleep['calendarDate'].unique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Floats to Int\n",
    "# combined_sleep_cleaned = combined_sleep_cleaned.astype({col: 'int' for col in combined_sleep_cleaned.select_dtypes(include='float').columns})\n",
    "# combined_sleep_cleaned.head()\n",
    "\n",
    "# Drop Columns\n",
    "cols_to_drop = ['retro','napList']\n",
    "combined_sleep_cleaned = combined_sleep_cleaned.drop(cols_to_drop,axis=1)\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45370aa8",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Sleep Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38463b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_sleep_cleaned.to_csv('PBI Data/Sleep_Cleaned_PBI.csv', index=False) #For PBI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253465a",
   "metadata": {},
   "source": [
    "## Load in Actute Training Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb140ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Acute Training Load Data\n",
    "atl_1_main = pd.read_json('Raw Data/Acute_Training_Load/MetricsAcuteTrainingLoad_20231103_20240211_117832404.json')\n",
    "atl_2_main = pd.read_json('Raw Data/Acute_Training_Load/MetricsAcuteTrainingLoad_20240211_20240521_117832404.json')\n",
    "atl_3_main = pd.read_json('Raw Data/Acute_Training_Load/MetricsAcuteTrainingLoad_20240521_20240829_117832404.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "atl_1 = atl_1_main.copy()\n",
    "atl_2 = atl_2_main.copy()\n",
    "atl_3 = atl_3_main.copy()\n",
    "\n",
    "# Assess atl_1 characteristcs\n",
    "print(atl_1.shape)\n",
    "print(atl_1.dtypes)\n",
    "\n",
    "# View Imported File\n",
    "atl_1\n",
    "#atl_2\n",
    "#atl_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out records with 'acwrStatus' == \"NONE\"\\\n",
    "atl_1_cleaned = atl_1[atl_1['acwrStatus'] != \"NONE\"]\n",
    "\n",
    "print(atl_1_cleaned.shape)\n",
    "atl_1_cleaned.head()\n",
    "\n",
    "# atl_1_cleaned.shape ###(170,10) Dropped 34 of 204 records\n",
    "\n",
    "### It appears the NONE records are from when I got the watch\n",
    "### None of the records in atl_2 or atl_3 have NONE values for acwrStatus\n",
    "### print(atl_2[atl_2['acwrStatus'] == \"NONE\"].shape)\n",
    "### print(atl_3[atl_3['acwrStatus'] == \"NONE\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8d644",
   "metadata": {},
   "source": [
    "### Assess Null Values in Acute Training Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts_1 = atl_1.isna().sum()\n",
    "null_counts_2 = atl_2.isna().sum()\n",
    "null_counts_3 = atl_3.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts_1)\n",
    "print(null_counts_2)\n",
    "print(null_counts_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83324d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess Datatypes\n",
    "atl_1_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee51f6",
   "metadata": {},
   "source": [
    "### Combine 3 Acute Training Load Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_atl = pd.concat([atl_1_cleaned, atl_2, atl_3], ignore_index=True)\n",
    "\n",
    "# Change 'calendarDate' to Datetime\n",
    "## combined_atl['calendarDate'] = pd.to_datetime(combined_atl['calendarDate']) ### Doesn't work as intended\n",
    "combined_atl['calendarDate'] = pd.to_datetime(combined_atl['timestamp']).dt.date\n",
    "\n",
    "### 'calendarDate' is in a really weird format so I am overriding it with the date from 'timestamp'\n",
    "combined_atl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'deviceId'\n",
    "combined_atl = combined_atl.drop('deviceId', axis=1) ### Comment out after first execution\n",
    "\n",
    "# View Results\n",
    "combined_atl.head()\n",
    "\n",
    "# Check for null values --> None\n",
    "## combined_atl.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0daee",
   "metadata": {},
   "source": [
    "### Assess Null Values in Combined Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b010904",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_atl.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4817630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Rows where dailyAcuteChronicWorkloadRatio is null\n",
    "# combined_atl[combined_atl['dailyAcuteChronicWorkloadRatio'].isna()]\n",
    "\n",
    "# Drop rows where dailyAcuteChronicWorkloadRatio is NaN\n",
    "combined_atl_cleaned = combined_atl[combined_atl['dailyAcuteChronicWorkloadRatio'].notna()]\n",
    "combined_atl_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119caf8",
   "metadata": {},
   "source": [
    "### Filter to 1 record for each 'calendarDate'.\n",
    "#### If there are multiple records for 1 Date, then keep the record with the maximum timestamp for that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'calendarDate' and get the index of the row with the greatest 'timestamp' for each day\n",
    "max_timestamp_idx = combined_atl_cleaned.groupby('calendarDate')['timestamp'].idxmax()\n",
    "\n",
    "# Select the rows with the maximum 'timestamp' for each day\n",
    "combined_atl_cleaned = combined_atl_cleaned.loc[max_timestamp_idx]\n",
    "\n",
    "# Verify the result\n",
    "combined_atl_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'acwrStatusFeedback' and 'timestamp' as these columns do not provide any value for ML model\n",
    "combined_atl_cleaned = combined_atl_cleaned.drop(['userProfilePK','acwrStatusFeedback','timestamp'],axis=1)\n",
    "combined_atl_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fca82",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Acute Training Load Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_atl_cleaned.to_csv('PBI Data/ATL_Cleaned_PBI.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c156103",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_atl_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b81be9",
   "metadata": {},
   "source": [
    "## Load in Max Met Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2310586",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in Max Met Data\n",
    "maxmet_1_main = pd.read_json('Raw Data/Max_Met_Data/MetricsMaxMetData_20231103_20240211_117832404.json')\n",
    "maxmet_2_main = pd.read_json('Raw Data/Max_Met_Data/MetricsMaxMetData_20240211_20240521_117832404.json')\n",
    "maxmet_3_main = pd.read_json('Raw Data/Max_Met_Data/MetricsMaxMetData_20240521_20240829_117832404.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "maxmet_1 = maxmet_1_main.copy()\n",
    "maxmet_2 = maxmet_2_main.copy()\n",
    "maxmet_3 = maxmet_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "maxmet_1\n",
    "## maxmet_2\n",
    "## maxmet_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c107ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess maxmet_1 characteristcs\n",
    "print(maxmet_1.shape)\n",
    "print(maxmet_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4877970",
   "metadata": {},
   "source": [
    "### Combine 3 Maxmet Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6559500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_maxmet = pd.concat([maxmet_1, maxmet_2, maxmet_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_maxmet['calendarDate'] = pd.to_datetime(combined_maxmet['calendarDate'])\n",
    "\n",
    "combined_maxmet\n",
    "## combined_maxmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ec0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_maxmet.shape)\n",
    "print(combined_maxmet.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c444c16",
   "metadata": {},
   "source": [
    "### Assess Null Values for Maxmet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_maxmet.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e250e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique value count across all columns\n",
    "unique_counts = combined_maxmet.nunique()\n",
    "\n",
    "# Display the unique value counts for each column\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ad2d7",
   "metadata": {},
   "source": [
    "### Create Function to drop columns with only one unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80038726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop columns with only one unique value\n",
    "def drop_single_value_columns(df):\n",
    "    # Identify columns with only one unique value\n",
    "    cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    \n",
    "    # Drop those columns\n",
    "    df_cleaned = df.drop(cols_to_drop, axis=1)\n",
    "    return df_cleaned\n",
    "\n",
    "# Example usage\n",
    "combined_maxmet = drop_single_value_columns(combined_maxmet)\n",
    "\n",
    "# Display the dataframe after dropping the columns\n",
    "combined_maxmet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4c7ac",
   "metadata": {},
   "source": [
    "### Filter to 1 record for each 'calendarDate'.\n",
    "#### If there are multiple records for 1 Date, then keep the record with the maximum updateTimestamp for that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'calendarDate' and get the index of the row with the greatest 'updateTimestamp' for each day\n",
    "max_timestamp_idx = combined_maxmet.groupby('calendarDate')['updateTimestamp'].idxmax()\n",
    "\n",
    "# Select the rows with the maximum 'timestamp' for each day\n",
    "maxmet_cleaned = combined_maxmet.loc[max_timestamp_idx]\n",
    "\n",
    "# Verify the result\n",
    "print(combined_maxmet.shape)\n",
    "print(maxmet_cleaned.shape)\n",
    "maxmet_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea4b91",
   "metadata": {},
   "source": [
    "### Add Records to Maxmet\n",
    "#### If there is a date of 08-01 and the next records is 08-07, I want to duplicate the 08-01 record as 08-02,08-03, etc. to fill in the gaps until 08-07\n",
    "#### This will allow me to have a row for every date when I join on 'calendarDate' later on to create my ML MASTER TBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e546c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete date range from the first to the last date\n",
    "date_range = pd.date_range(start=maxmet_cleaned['calendarDate'].min(), \n",
    "                           end=maxmet_cleaned['calendarDate'].max())\n",
    "\n",
    "# Reindex the DataFrame with the new date range\n",
    "maxmet_cleaned_2 = maxmet_cleaned.set_index('calendarDate').reindex(date_range)\n",
    "\n",
    "# Forward fill the missing values to copy the previous dayâ€™s record\n",
    "maxmet_cleaned_2 = maxmet_cleaned_2.ffill()\n",
    "\n",
    "# Reset the index to bring 'calendarDate' back as a column\n",
    "maxmet_cleaned_2 = maxmet_cleaned_2.reset_index().rename(columns={'index': 'calendarDate'})\n",
    "\n",
    "# Verify the result\n",
    "print(maxmet_cleaned.shape)\n",
    "print(maxmet_cleaned_2.shape)\n",
    "maxmet_cleaned_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'updateTimestamp' and 'sport' as these columns do not provide much value\n",
    "maxmet_cleaned_2 = maxmet_cleaned_2.drop(['updateTimestamp','sport'],axis=1)\n",
    "maxmet_cleaned_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d030f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'vo2MaxValue' and 'maxMet' values with the value from the preceeding row\n",
    "maxmet_cleaned_2.loc[:, 'vo2MaxValue'] = maxmet_cleaned_2['vo2MaxValue'].ffill()\n",
    "maxmet_cleaned_2.loc[:, 'maxMet'] = maxmet_cleaned_2['maxMet'].ffill()\n",
    "\n",
    "### This is not doing anything now, but if there are missing values later, this will fix the problem.\n",
    "### May need to move this code prior to the generation of additional calendar Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efea4e",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Max Met Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxmet_cleaned_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "maxmet_cleaned_2.to_csv('PBI Data/MaxMet_Cleaned_PBI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53c4a3",
   "metadata": {},
   "source": [
    "## Load in Race Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5828b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in Race Prediction Data\n",
    "racepred_1_main = pd.read_json('Raw Data/Race_Predictions/RunRacePredictions_20231103_20240211_117832404.json')\n",
    "racepred_2_main = pd.read_json('Raw Data/Race_Predictions/RunRacePredictions_20240211_20240521_117832404.json')\n",
    "racepred_3_main = pd.read_json('Raw Data/Race_Predictions/RunRacePredictions_20240521_20240829_117832404.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "racepred_1 = racepred_1_main.copy()\n",
    "racepred_2 = racepred_2_main.copy()\n",
    "racepred_3 = racepred_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "racepred_1.head()\n",
    "\n",
    "# Preliminary Analysis\n",
    "\n",
    "### There are multiple rows per Day. It may be best to take the average for the day. \n",
    "### Well maybe not because if the garmin algorithm is causing it to change intra-day, \n",
    "### then our algorithm should do the same thing. \n",
    "### Maybe start with a daily average and then progress from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess racepred_1 characteristcs\n",
    "print(racepred_1.shape)\n",
    "print(racepred_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad24703",
   "metadata": {},
   "source": [
    "### Combine 3 Race Prediction Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fd7fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_racepred = pd.concat([racepred_1, racepred_2, racepred_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_racepred['calendarDate'] = pd.to_datetime(combined_racepred['calendarDate'])\n",
    "\n",
    "# View Dataframe\n",
    "combined_racepred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefab094",
   "metadata": {},
   "source": [
    "### Clean up remaining data types for Race Predication Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade3fb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Define a function to convert seconds to a timedelta\n",
    "# def to_timedelta(seconds):\n",
    "#     return pd.Timedelta(seconds=seconds)\n",
    "\n",
    "# # List of columns to convert\n",
    "# columns_to_convert = ['raceTime5K', 'raceTime10K', 'raceTimeHalf', 'raceTimeMarathon']\n",
    "\n",
    "# # Apply the formatting function to each race time column\n",
    "# for column in columns_to_convert:\n",
    "#     combined_racepred[column] = combined_racepred[column].apply(to_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to convert seconds to a timedelta\n",
    "# def to_timedelta(seconds):\n",
    "#     return pd.Timedelta(seconds=seconds)\n",
    "\n",
    "# # List of original columns to convert\n",
    "# columns_to_convert = ['raceTime5K', 'raceTime10K', 'raceTimeHalf', 'raceTimeMarathon']\n",
    "\n",
    "# # Create new columns with the converted values\n",
    "# for column in columns_to_convert:\n",
    "#     # Create new column names by appending '_timedelta' to the original column names\n",
    "#     new_column_name = f\"{column}_timedelta\"\n",
    "#     combined_racepred[new_column_name] = combined_racepred[column].apply(to_timedelta)\n",
    "\n",
    "# # Display the dataframe with the new columns\n",
    "# combined_racepred[['raceTime5K', 'raceTime5K_timedelta', \n",
    "#                    'raceTime10K', 'raceTime10K_timedelta', \n",
    "#                    'raceTimeHalf', 'raceTimeHalf_timedelta', \n",
    "#                    'raceTimeMarathon', 'raceTimeMarathon_timedelta']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_racepred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb5098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_racepred.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cd8b07",
   "metadata": {},
   "source": [
    "### Drop Invaluable columns from Race Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ddcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Columns to drop\n",
    "columns_to_drop = ['deviceId'] ##, 'timestamp']\n",
    "\n",
    "racepred_cleaned = combined_racepred.drop(columns_to_drop, axis=1)\n",
    "\n",
    "racepred_cleaned.shape ## (917,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a38ba",
   "metadata": {},
   "source": [
    "### Group by calendarDate and select the MIN race time for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d364dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'calendarDate' and find the minimum race times\n",
    "min_race_times = racepred_cleaned.groupby('calendarDate').agg({\n",
    "    'raceTime5K': 'min',\n",
    "    'raceTime10K': 'min',\n",
    "    'raceTimeHalf': 'min',\n",
    "    'raceTimeMarathon': 'min'\n",
    "}).reset_index()\n",
    "\n",
    "min_race_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the race times to timedeltas (values are in seconds)\n",
    "min_race_times['raceTime5K_timedelta'] = pd.to_timedelta(min_race_times['raceTime5K'], unit='s')\n",
    "min_race_times['raceTime10K_timedelta'] = pd.to_timedelta(min_race_times['raceTime10K'], unit='s')\n",
    "min_race_times['raceTimeHalf_timedelta'] = pd.to_timedelta(min_race_times['raceTimeHalf'], unit='s')\n",
    "min_race_times['raceTimeMarathon_timedelta'] = pd.to_timedelta(min_race_times['raceTimeMarathon'], unit='s')\n",
    "\n",
    "# View Results\n",
    "min_race_times\n",
    "\n",
    "## min_race_times['calendarDate'].unique().value_counts()\n",
    "## min_race_times.shape (280,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0162b56",
   "metadata": {},
   "source": [
    "### Conversion of Time-oriented fields to Duration in PBI was not working as expected\n",
    "#### Drop '0 Days' for Time-oriented fields and format as strings for PBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f33dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert timedelta to string and clean '0 days ' part\n",
    "def clean_timedelta_to_string(val):\n",
    "    if isinstance(val, pd.Timedelta):\n",
    "        # Convert timedelta to string and remove '0 days ' part\n",
    "        clean_value = str(val).split(' ')[-1]  # Keep only the 'hh:mm:ss' part\n",
    "        return clean_value  # Return the string in hh:mm:ss format\n",
    "    return val  # Return the original value if not a timedelta\n",
    "\n",
    "# Create a new DataFrame df_pbi by copying df_cleaned\n",
    "min_race_times_pbi = min_race_times.copy()\n",
    "\n",
    "# List of columns to clean\n",
    "cols = ['raceTime5K_timedelta','raceTime10K_timedelta','raceTimeHalf_timedelta','raceTimeMarathon_timedelta']\n",
    "\n",
    "# Apply the function to the specified columns in df_pbi to remove '0 days'\n",
    "for col in cols:\n",
    "    min_race_times_pbi[col] = min_race_times_pbi[col].apply(clean_timedelta_to_string)\n",
    "\n",
    "# Check the data types after cleaning to ensure they are 'object' (string)\n",
    "print(min_race_times_pbi.dtypes)\n",
    "\n",
    "# Display the updated dataframe\n",
    "min_race_times_pbi.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Year, Month, Quarter, and Day columns\n",
    "min_race_times_pbi['Year'] = min_race_times_pbi['calendarDate'].dt.year\n",
    "min_race_times_pbi['Month'] = min_race_times_pbi['calendarDate'].dt.month\n",
    "min_race_times_pbi['MonthName'] = min_race_times_pbi['calendarDate'].dt.month_name()\n",
    "min_race_times_pbi['Quarter'] = min_race_times_pbi['calendarDate'].dt.quarter\n",
    "min_race_times_pbi['Day'] = min_race_times_pbi['calendarDate'].dt.day\n",
    "\n",
    "\n",
    "# Check the resulting columns\n",
    "print(min_race_times_pbi[['calendarDate', 'Year', 'Month', 'MonthName', 'Quarter', 'Day']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder Columns: 'Weekly_Cumulative_Mins','Weekly_Mins_Prior_to_Run','Monthly_Cumulative_Mins','Monthly_Mins_Prior_to_Run'\n",
    "columns = min_race_times_pbi.columns.to_list()\n",
    "\n",
    "# Get the index of the 'Time' column\n",
    "distance_index = columns.index('calendarDate')\n",
    "\n",
    "# Insert 'Year' right after 'calendarDate'\n",
    "columns.insert(distance_index + 1, columns.pop(columns.index('Year')))\n",
    "\n",
    "# Insert 'Month' right after 'Year'\n",
    "columns.insert(distance_index + 2, columns.pop(columns.index('Month')))\n",
    "\n",
    "# Insert 'MonthName' right after 'Month'\n",
    "columns.insert(distance_index + 3, columns.pop(columns.index('MonthName')))\n",
    "\n",
    "# Insert 'Quarter' right after 'Month'\n",
    "columns.insert(distance_index + 4, columns.pop(columns.index('Quarter')))\n",
    "\n",
    "# Insert 'Day' right after 'Quarter'\n",
    "columns.insert(distance_index + 5, columns.pop(columns.index('Day')))\n",
    "\n",
    "# Update df_cleaned with new Column Order\n",
    "min_race_times_pbi = min_race_times_pbi[columns]\n",
    "\n",
    "# Check the updated DataFrame structure\n",
    "min_race_times_pbi.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca65ac",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Race Prediction Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "min_race_times_pbi.to_csv('PBI Data/RacePredictions_Cleaned_PBI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a8523",
   "metadata": {},
   "source": [
    "## Loading in Training History Data\n",
    "#### Do this after cleaning up Race Prediction Data for PBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Summarized Activity Data\n",
    "training_hist_1_main = pd.read_json('Raw Data/Training_History/TrainingHistory_20231103_20240211_117832404.json')\n",
    "training_hist_2_main = pd.read_json('Raw Data/Training_History/TrainingHistory_20240211_20240521_117832404.json')\n",
    "training_hist_3_main = pd.read_json('Raw Data/Training_History/TrainingHistory_20240521_20240829_117832404.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "training_hist_1 = training_hist_1_main.copy()\n",
    "training_hist_2 = training_hist_2_main.copy()\n",
    "training_hist_3 = training_hist_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "training_hist_3\n",
    "\n",
    "# Preliminary Analysis\n",
    "\n",
    "### There are multiple records per day as well. Need to factor these changes in.\n",
    "### Maybe it is better to get a daily prediction, and then I can circle back to intra day updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f158d92",
   "metadata": {},
   "source": [
    "### Combine 3 Training History Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42df721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_training_hist = pd.concat([training_hist_1, training_hist_2, training_hist_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_training_hist['calendarDate'] = pd.to_datetime(combined_training_hist['calendarDate'])\n",
    "\n",
    "combined_training_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess combined_training_hist characteristcs\n",
    "print(combined_training_hist.shape)\n",
    "print(combined_training_hist.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba6372",
   "metadata": {},
   "source": [
    "### Group By 'calendarDate' and Select the Last Training Status of Each Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'calendarDate' and get the index of rows with the maximum timestamp\n",
    "idx = combined_training_hist.groupby('calendarDate')['timestamp'].idxmax()\n",
    "\n",
    "# Select the rows with the maximum timestamp for each day\n",
    "combined_training_hist_cleaned = combined_training_hist.loc[idx]\n",
    "\n",
    "# View Results\n",
    "combined_training_hist_cleaned\n",
    "\n",
    "# Confirm that there is only 1 row per 'calendarDate'\n",
    "## combined_training_hist_cleaned['calendarDate'].unique().value_counts()\n",
    "\n",
    "# Assess combined_training_hist_cleaned characteristics\n",
    "## print(combined_training_hist_cleaned.shape) (280,9)\n",
    "## print(combined_training_hist_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_training_hist_cleaned.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records where 'trainingStatus' is 'NO_STATUS'\n",
    "combined_training_hist_cleaned = combined_training_hist_cleaned[combined_training_hist_cleaned['trainingStatus'] != 'NO_STATUS']\n",
    "combined_training_hist_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf050cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Insignificant Columns\n",
    "combined_training_hist_cleaned = combined_training_hist_cleaned.drop(['sport'\n",
    "                                                                      ,'subSport'\n",
    "                                                                      ,'deviceId'\n",
    "                                                                      ,'timestamp'\n",
    "                                                                      ,'trainingStatus2FeedbackPhrase'\n",
    "                                                                      ,'userProfilePK']\n",
    "                                                                      , axis=1)\n",
    "combined_training_hist_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f762c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_training_hist_cleaned.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Final Dataframe\n",
    "combined_training_hist_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e57b9",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Training History Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_training_hist_cleaned.to_csv('PBI Data/TrainingHistory_Cleaned_PBI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45591912",
   "metadata": {},
   "source": [
    "## Load in UDS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in UDS Data\n",
    "uds_1_main = pd.read_json('Raw Data/UDS_Data/UDSFile_2023-10-14_2024-01-22.json')\n",
    "uds_2_main = pd.read_json('Raw Data/UDS_Data/UDSFile_2024-01-22_2024-05-01.json')\n",
    "uds_3_main = pd.read_json('Raw Data/UDS_Data/UDSFile_2024-05-01_2024-08-09.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "uds_1 = uds_1_main.copy()\n",
    "uds_2 = uds_2_main.copy()\n",
    "uds_3 = uds_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "uds_1.head()\n",
    "\n",
    "# Assess uds_1 characteristcs\n",
    "## print(uds_1.shape)\n",
    "## print(uds_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30f6df",
   "metadata": {},
   "source": [
    "### Combine 3 UDS Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d86a4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_uds = pd.concat([uds_1, uds_2, uds_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_uds['calendarDate'] = pd.to_datetime(combined_uds['calendarDate'])\n",
    "\n",
    "combined_uds\n",
    "\n",
    "# Assess combined_uds characteristcs\n",
    "# print(combined_uds.shape)\n",
    "# print(combined_uds.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c95bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_uds.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862b932",
   "metadata": {},
   "source": [
    "### Extract data from dictionary type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27430dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary-like values in 'allDayStress' and specify suffixes to avoid column overlap\n",
    "combined_uds = combined_uds.join(combined_uds['allDayStress'].apply(pd.Series), rsuffix='_stress')\n",
    "\n",
    "# Drop Columns\n",
    "combined_uds = combined_uds.drop(['allDayStress','calendarDate_stress','userProfilePK_stress'], axis=1)\n",
    "\n",
    "# View Results\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a70c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each list of dictionaries in place without exploding the DataFrame\n",
    "expanded_df = pd.json_normalize(combined_uds['aggregatorList'])\n",
    "\n",
    "# Combine the new columns back into the original dataframe without altering row count\n",
    "combined_uds = pd.concat([combined_uds, expanded_df], axis=1)\n",
    "\n",
    "# Drop the original list column if necessary\n",
    "combined_uds = combined_uds.drop(columns=['aggregatorList'])\n",
    "\n",
    "# Drop Last 2 columns as the 3rd to last contains all necessary information\n",
    "combined_uds = combined_uds.iloc[:, :-2]\n",
    "\n",
    "# Rename '0' column \n",
    "combined_uds = combined_uds.rename(columns={combined_uds.columns[-1]: 'Total_stress_data'})\n",
    "combined_uds.head()\n",
    "\n",
    "# View Results\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1316b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'bodyBattery' column into separate columns\n",
    "combined_uds = combined_uds.join(combined_uds['Total_stress_data'].apply(pd.Series), rsuffix='_stress')\n",
    "\n",
    "# Drop 'Total_stress_data' Column\n",
    "combined_uds = combined_uds.drop(['Total_stress_data'], axis=1)\n",
    "\n",
    "combined_uds.head()\n",
    "## combined_uds.shape (259,68)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c41b02",
   "metadata": {},
   "source": [
    "### Not using this code at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bea0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the dictionary like values in 'bodyBattery' column into separate columns\n",
    "# combined_uds = combined_uds.join(combined_uds['bodyBattery'].apply(pd.Series), rsuffix='_battery')\n",
    "\n",
    "# # Drop 'bodyBattery' Column\n",
    "# combined_uds = combined_uds.drop(['bodyBattery'], axis=1)\n",
    "\n",
    "# # Drop Other Duplicate Columns\n",
    "# combined_uds = combined_uds.drop(['userProfilePK_battery','calendarDate_battery'], axis=1)\n",
    "# combined_uds.head()\n",
    "\n",
    "# combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52615e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize each list of dictionaries in place without exploding the DataFrame\n",
    "# expanded_df = pd.json_normalize(combined_uds['bodyBatteryStatList'])\n",
    "\n",
    "# # Combine the new columns back into the original dataframe without altering row count\n",
    "# combined_uds = pd.concat([combined_uds, expanded_df], axis=1)\n",
    "\n",
    "# # Drop the original list column if necessary\n",
    "# combined_uds = combined_uds.drop(columns=['bodyBatteryStatList'])\n",
    "\n",
    "# combined_uds.head()\n",
    "# ## combined_uds.shape (259,78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop other insignificant columns\n",
    "combined_uds = combined_uds.drop(['uuid'\n",
    "                                  ,'userProfilePK'\n",
    "                                  ,'wellnessStartTimeGmt'\n",
    "                                  ,'wellnessEndTimeGmt'\n",
    "                                  ,'wellnessStartTimeLocal'\n",
    "                                  ,'wellnessEndTimeLocal'\n",
    "                                  ,'includesWellnessData'\n",
    "                                  ,'includesActivityData'\n",
    "                                  ,'includesCalorieConsumedData'\n",
    "                                  ,'includesSingleMeasurement'\n",
    "                                  ,'includesContinuousMeasurement'\n",
    "                                  ,'includesAllDayPulseOx'\n",
    "                                  ,'includesSleepPulseOx'\n",
    "                                  ,'source'\n",
    "                                  ,'userFloorsAscendedGoal'\n",
    "                                  ,'durationInMilliseconds'\n",
    "                                  ,'wellnessKilocalories'\n",
    "                                  ,'remainingKilocalories' # Same Values as 'wellnessKilocalories'\n",
    "                                  ,'wellnessTotalKilocalories' # Same Values as 'wellnessKilocalories'\n",
    "                                  ,'wellnessActiveKilocalories'\n",
    "                                  ,'dailyStepGoal'\n",
    "                                  ,'wellnessDistanceMeters'\n",
    "                                  ,'userIntensityMinutesGoal'\n",
    "                                  ,'minAvgHeartRate'\n",
    "                                  ,'maxAvgHeartRate'\n",
    "                                  ,'version'\n",
    "                                  ,'restingCaloriesFromActivity'\n",
    "                                  ,'restingHeartRateTimestamp'\n",
    "                                 # ,'hydration'\n",
    "                                  ,'dailyTotalFromEpochData'\n",
    "                                  ,'type'\n",
    "                                  ,'uncategorizedDuration'\n",
    "                                  ,'totalDuration'\n",
    "                                  ,'lowDuration'\n",
    "                                  ,'bodyBattery'\n",
    "                                  ,'floorsAscendedInMeters'\n",
    "                                  ,'floorsDescendedInMeters'\n",
    "                                  ,'averageMonitoringEnvironmentAltitude'\n",
    "                                  ,'respiration'\n",
    "                                  ,'hydration'\n",
    "                                  ,'bodyBatteryFeedback'\n",
    "                                 ], axis=1)\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique value count across all columns\n",
    "unique_counts = combined_uds.nunique()\n",
    "\n",
    "# Display the unique value counts for each column\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0.0 in the 'isVigorousDay' column\n",
    "combined_uds['isVigorousDay'] = combined_uds['isVigorousDay'].fillna(0.0)\n",
    "\n",
    "# Verify the changes\n",
    "combined_uds['isVigorousDay'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68496db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in each column with the column's mean\n",
    "combined_uds = combined_uds.fillna(combined_uds.mean(numeric_only=True))\n",
    "combined_uds\n",
    "\n",
    "# Verify that the missing values have been filled\n",
    "combined_uds.isna().sum() # This should show 0 for columns that had missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c43f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert all 'float' type columns to 'int'\n",
    "combined_uds = combined_uds.astype({col: 'int' for col in combined_uds.select_dtypes(include='float').columns})\n",
    "combined_uds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Year, Month, Quarter, and Day columns\n",
    "combined_uds['Year'] = combined_uds['calendarDate'].dt.year\n",
    "combined_uds['Month'] = combined_uds['calendarDate'].dt.month\n",
    "combined_uds['MonthName'] = combined_uds['calendarDate'].dt.month_name()\n",
    "combined_uds['Quarter'] = combined_uds['calendarDate'].dt.quarter\n",
    "combined_uds['Day'] = combined_uds['calendarDate'].dt.day\n",
    "\n",
    "\n",
    "# Check the resulting columns\n",
    "print(combined_uds[['calendarDate', 'Year', 'Month', 'MonthName', 'Quarter', 'Day']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb04ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder Columns: 'Weekly_Cumulative_Mins','Weekly_Mins_Prior_to_Run','Monthly_Cumulative_Mins','Monthly_Mins_Prior_to_Run'\n",
    "columns = combined_uds.columns.to_list()\n",
    "\n",
    "# Get the index of the 'Time' column\n",
    "distance_index = columns.index('calendarDate')\n",
    "\n",
    "# Insert 'Year' right after 'calendarDate'\n",
    "columns.insert(distance_index + 1, columns.pop(columns.index('Year')))\n",
    "\n",
    "# Insert 'Month' right after 'Year'\n",
    "columns.insert(distance_index + 2, columns.pop(columns.index('Month')))\n",
    "\n",
    "# Insert 'MonthName' right after 'Month'\n",
    "columns.insert(distance_index + 3, columns.pop(columns.index('MonthName')))\n",
    "\n",
    "# Insert 'Quarter' right after 'Month'\n",
    "columns.insert(distance_index + 4, columns.pop(columns.index('Quarter')))\n",
    "\n",
    "# Insert 'Day' right after 'Quarter'\n",
    "columns.insert(distance_index + 5, columns.pop(columns.index('Day')))\n",
    "\n",
    "# Update df_cleaned with new Column Order\n",
    "combined_uds = combined_uds[columns]\n",
    "\n",
    "# Check the updated DataFrame structure\n",
    "combined_uds.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f5544",
   "metadata": {},
   "source": [
    "### Save Pre-Processed UDS Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd7494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_uds.to_csv('PBI Data/UDS_Cleaned_PBI.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
