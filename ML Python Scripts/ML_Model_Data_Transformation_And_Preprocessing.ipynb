{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show All Columns and Rows for Dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db0f2",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f0060",
   "metadata": {},
   "source": [
    "## Load in Actute Training Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Acute Training Load Data\n",
    "atl_1_main = pd.read_json('Raw Data/Acute_Training_Load/MetricsAcuteTrainingLoad_20231103_20240211_117832404.json')\n",
    "atl_2_main = pd.read_json('Raw Data/Acute_Training_Load/MetricsAcuteTrainingLoad_20240211_20240521_117832404.json')\n",
    "atl_3_main = pd.read_json('Raw Data/Acute_Training_Load/MetricsAcuteTrainingLoad_20240521_20240829_117832404.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "atl_1 = atl_1_main.copy()\n",
    "atl_2 = atl_2_main.copy()\n",
    "atl_3 = atl_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "atl_1\n",
    "#atl_2\n",
    "#atl_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b30f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess atl_1 characteristcs\n",
    "print(atl_1.shape)\n",
    "print(atl_1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Data\n",
    "## atl_1\n",
    "\n",
    "# Filter out records with 'acwrStatus' == \"NONE\"\\\n",
    "atl_1_cleaned = atl_1[atl_1['acwrStatus'] != \"NONE\"]\n",
    "\n",
    "atl_1_cleaned.head()\n",
    "\n",
    "# atl_1_cleaned.shape ###(170,10) Dropped 34 of 204 records\n",
    "\n",
    "### It appears the NONE records are from when I got the watch\n",
    "### None of the records in atl_2 or atl_3 have NONE values for acwrStatus\n",
    "### print(atl_2[atl_2['acwrStatus'] == \"NONE\"].shape)\n",
    "### print(atl_3[atl_3['acwrStatus'] == \"NONE\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8d644",
   "metadata": {},
   "source": [
    "#### Assess Null Values in Acute Training Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts_1 = atl_1.isna().sum()\n",
    "null_counts_2 = atl_2.isna().sum()\n",
    "null_counts_3 = atl_3.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts_1)\n",
    "print(null_counts_2)\n",
    "print(null_counts_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcefa89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assess Datatypes\n",
    "atl_1_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee51f6",
   "metadata": {},
   "source": [
    "### Combine 3 Acute Training Load Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_atl = pd.concat([atl_1_cleaned, atl_2, atl_3], ignore_index=True)\n",
    "\n",
    "# Change 'calendarDate' to Datetime\n",
    "## combined_atl['calendarDate'] = pd.to_datetime(combined_atl['calendarDate']) ### Doesn't work as intended\n",
    "combined_atl['calendarDate'] = pd.to_datetime(combined_atl['timestamp']).dt.date\n",
    "\n",
    "### 'calendarDate' is in a really weird format so I am overriding it with the date from 'timestamp'\n",
    "combined_atl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9366c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'deviceId'\n",
    "combined_atl = combined_atl.drop('deviceId', axis=1) ### Comment out after first execution\n",
    "\n",
    "# View Results\n",
    "combined_atl.head()\n",
    "\n",
    "# Check for null values --> None\n",
    "## combined_atl.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0daee",
   "metadata": {},
   "source": [
    "### Assess Null Values in Combined Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_atl.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd21165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where dailyAcuteChronicWorkloadRatio is NaN\n",
    "combined_atl_cleaned = combined_atl[combined_atl['dailyAcuteChronicWorkloadRatio'].notna()]\n",
    "combined_atl_cleaned\n",
    "\n",
    "### PROBLEM: There are multiple records for each 'calendarDate'. I only want 1 record for the MAX 'timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119caf8",
   "metadata": {},
   "source": [
    "### Filter to 1 record for each 'calendarDate'.\n",
    "#### If there are multiple records for 1 Date, then keep the record with the maximum timestamp for that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'calendarDate' and get the index of the row with the greatest 'timestamp' for each day\n",
    "max_timestamp_idx = combined_atl_cleaned.groupby('calendarDate')['timestamp'].idxmax()\n",
    "\n",
    "# Select the rows with the maximum 'timestamp' for each day\n",
    "combined_atl_cleaned = combined_atl_cleaned.loc[max_timestamp_idx]\n",
    "\n",
    "# Verify the result\n",
    "combined_atl_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'acwrStatusFeedback' and 'timestamp' as these columns do not provide any value for ML model\n",
    "combined_atl_cleaned = combined_atl_cleaned.drop(['userProfilePK','acwrStatusFeedback','timestamp'],axis=1)\n",
    "combined_atl_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fca82",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Acute Training Load Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_atl_cleaned.to_csv('Processed_Data/Acute_Training_Load_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75623132",
   "metadata": {},
   "source": [
    "## Load in Bio Metrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81111a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in Bio Metric Data -> Not going to use for initial model.\n",
    "\n",
    "# bio_profile_main = pd.read_json('Raw Data/Bio_Metrics/117832404_userBioMetricProfileData.json')  ### Not Useful\n",
    "# bio_metrics_main = pd.read_json('Raw Data/Bio_Metrics/117832404_userBioMetrics.json')  ### Not Useful\n",
    "\n",
    "## Make Copies of Dataframes\n",
    "# bio_profile = bio_profile_main.copy()\n",
    "# bio_metrics = bio_metrics_main.copy()\n",
    "\n",
    "# # View Imported File\n",
    "# bio_profile.head() ### 1 Row of Useless Data for ML Model\n",
    "# bio_metrics.head() ### Useless Data for ML Model\n",
    "\n",
    "# # Convert the dictionary-like values in 'allDayStress' and specify suffixes to avoid column overlap\n",
    "# bio_metrics = bio_metrics.join(bio_metrics['metaData'].apply(pd.Series), rsuffix='_meta')\n",
    "\n",
    "# combined_uds.head()\n",
    "\n",
    "# # Drop Columns\n",
    "# bio_metrics = bio_metrics.drop(['allDayStress','calendarDate_stress','userProfilePK_stress'], axis=1)\n",
    "\n",
    "# # View Results\n",
    "# bio_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c92cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess Bio Metrics Data characteristcs\n",
    "\n",
    "## print(bio_metrics.shape)\n",
    "## print(bio_metrics.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483617fb",
   "metadata": {},
   "source": [
    "## Load in Fitness Age Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in Fitness Age Data -> Not going to use for initial model. Maybe be valuable to bring in current age vs fitness age\n",
    "# fit_age_main = pd.read_json('Raw Data/Fitness Age and Heart Rate Zones/117832404_fitnessAgeData.json')\n",
    "\n",
    "## Make Copies of Dataframes\n",
    "# fit_age = fit_age_main.copy()\n",
    "\n",
    "## View Imported File\n",
    "# fit_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess fit_age characteristcs\n",
    "## print(fit_age.shape)\n",
    "## print(fit_age.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27723a2",
   "metadata": {},
   "source": [
    "## Load in Heart Rate Zone Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a578ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in HR Zone Data\n",
    "hr_zones_main = pd.read_json('Raw Data/Fitness Age and Heart Rate Zones/117832404_heartRateZones.json')\n",
    "\n",
    "# Make Copy of Dataframe\n",
    "hr_zones = hr_zones_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "hr_zones\n",
    "\n",
    "# Preliminary Analysis\n",
    "\n",
    "### Do I need to build an archive for this?\n",
    "### I can create a HR_Zone variable in MASTER table that takes the average HR and places the run in one of the zones. \n",
    "### It would be nice to see the % I speed in each zone in each run.\n",
    "### I'm not sure if that is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess fit_age characteristcs\n",
    "print(hr_zones.shape)\n",
    "print(hr_zones.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f92741",
   "metadata": {},
   "source": [
    "## Load in Hydration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf5a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in Hydration Data -> Not going to use for initial model.\n",
    "## hydro_1 = pd.read_json('Raw Data/Hydration_Data/HydrationLogFile_2023-10-14_2024-01-22.json')\n",
    "## hydro_2 = pd.read_json('Raw Data/Hydration_Data/HydrationLogFile_2024-01-22_2024-05-01.json')\n",
    "## hydro_3 = pd.read_json('Raw Data/Hydration_Data/HydrationLogFile_2024-05-01_2024-08-09.json')\n",
    "\n",
    "# View Imported File\n",
    "## hydro_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d84eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess hydro_1 characteristcs\n",
    "## print(hydro_1.shape)\n",
    "## print(hydro_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b81be9",
   "metadata": {},
   "source": [
    "## Load in Max Met Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2310586",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in Max Met Data\n",
    "maxmet_1_main = pd.read_json('Raw Data/Max_Met_Data/MetricsMaxMetData_20231103_20240211_117832404.json')\n",
    "maxmet_2_main = pd.read_json('Raw Data/Max_Met_Data/MetricsMaxMetData_20240211_20240521_117832404.json')\n",
    "maxmet_3_main = pd.read_json('Raw Data/Max_Met_Data/MetricsMaxMetData_20240521_20240829_117832404.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "maxmet_1 = maxmet_1_main.copy()\n",
    "maxmet_2 = maxmet_2_main.copy()\n",
    "maxmet_3 = maxmet_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "maxmet_1\n",
    "## maxmet_2\n",
    "## maxmet_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess maxmet_1 characteristcs\n",
    "print(maxmet_1.shape)\n",
    "print(maxmet_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4877970",
   "metadata": {},
   "source": [
    "### Combine 3 Maxmet Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6559500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_maxmet = pd.concat([maxmet_1, maxmet_2, maxmet_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_maxmet['calendarDate'] = pd.to_datetime(combined_maxmet['calendarDate'])\n",
    "\n",
    "combined_maxmet\n",
    "## combined_maxmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ec0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_maxmet.shape)\n",
    "print(combined_maxmet.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c444c16",
   "metadata": {},
   "source": [
    "### Assess Null Values for Maxmet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_maxmet.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ecba9",
   "metadata": {},
   "source": [
    "### Drop Invaluable Columns from Maxmet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d079b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Dataframe\n",
    "## combined_maxmet\n",
    "\n",
    "# Specify Columns to drop\n",
    "columns_to_drop = ['deviceId', 'subSport', 'maxMetCategory', 'calibratedData']\n",
    "\n",
    "maxmet_cleaned = combined_maxmet.drop(columns_to_drop, axis=1)\n",
    "maxmet_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4c7ac",
   "metadata": {},
   "source": [
    "### Filter to 1 record for each 'calendarDate'.\n",
    "#### If there are multiple records for 1 Date, then keep the record with the maximum updateTimestamp for that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'calendarDate' and get the index of the row with the greatest 'updateTimestamp' for each day\n",
    "max_timestamp_idx = maxmet_cleaned.groupby('calendarDate')['updateTimestamp'].idxmax()\n",
    "\n",
    "# Select the rows with the maximum 'timestamp' for each day\n",
    "maxmet_cleaned = maxmet_cleaned.loc[max_timestamp_idx]\n",
    "\n",
    "# Verify the result\n",
    "maxmet_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea4b91",
   "metadata": {},
   "source": [
    "### Add Records to Maxmet\n",
    "#### If there is a date of 08-01 and the next records is 08-07, I want to duplicate the 08-01 record as 08-02,08-03, etc. to fill in the gaps until 08-07\n",
    "#### This will allow me to have a row for every date when I join on 'calendarDate' later on to create my ML MASTER TBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e546c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete date range from the first to the last date\n",
    "date_range = pd.date_range(start=maxmet_cleaned['calendarDate'].min(), \n",
    "                           end=maxmet_cleaned['calendarDate'].max())\n",
    "\n",
    "# Reindex the DataFrame with the new date range\n",
    "maxmet_cleaned_2 = maxmet_cleaned.set_index('calendarDate').reindex(date_range)\n",
    "\n",
    "# Forward fill the missing values to copy the previous dayâ€™s record\n",
    "maxmet_cleaned_2 = maxmet_cleaned_2.ffill()\n",
    "\n",
    "# Reset the index to bring 'calendarDate' back as a column\n",
    "maxmet_cleaned_2 = maxmet_cleaned_2.reset_index().rename(columns={'index': 'calendarDate'})\n",
    "\n",
    "# Verify the result\n",
    "maxmet_cleaned_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'updateTimestamp' and 'sport' as these columns do not provide any value for ML model\n",
    "maxmet_cleaned_2 = maxmet_cleaned_2.drop(['userProfilePK','updateTimestamp','sport'],axis=1)\n",
    "maxmet_cleaned_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d030f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'vo2MaxValue' and 'maxMet' values with the value from the preceeding row\n",
    "maxmet_cleaned_2.loc[:, 'vo2MaxValue'] = maxmet_cleaned_2['vo2MaxValue'].ffill()\n",
    "maxmet_cleaned_2.loc[:, 'maxMet'] = maxmet_cleaned_2['maxMet'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efea4e",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Max Met Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "maxmet_cleaned_2.to_csv('Processed_Data/MaxMet_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53c4a3",
   "metadata": {},
   "source": [
    "## Load in Race Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5828b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in Race Prediction Data\n",
    "racepred_1_main = pd.read_json('Raw Data/Race_Predictions/RunRacePredictions_20231103_20240211_117832404.json')\n",
    "racepred_2_main = pd.read_json('Raw Data/Race_Predictions/RunRacePredictions_20240211_20240521_117832404.json')\n",
    "racepred_3_main = pd.read_json('Raw Data/Race_Predictions/RunRacePredictions_20240521_20240829_117832404.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "racepred_1 = racepred_1_main.copy()\n",
    "racepred_2 = racepred_2_main.copy()\n",
    "racepred_3 = racepred_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "racepred_1.head()\n",
    "\n",
    "# Preliminary Analysis\n",
    "\n",
    "### There are multiple rows per Day. It may be best to take the average for the day. \n",
    "### Well maybe not because if the garmin algorithm is causing it to change intra-day, \n",
    "### then our algorithm should do the same thing. \n",
    "### Maybe start with a daily average and then progress from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess racepred_1 characteristcs\n",
    "print(racepred_1.shape)\n",
    "print(racepred_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad24703",
   "metadata": {},
   "source": [
    "### Combine 3 Race Prediction Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fd7fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_racepred = pd.concat([racepred_1, racepred_2, racepred_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_racepred['calendarDate'] = pd.to_datetime(combined_racepred['calendarDate'])\n",
    "\n",
    "# View Dataframe\n",
    "combined_racepred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefab094",
   "metadata": {},
   "source": [
    "### Clean up remaining data types for Race Predication Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade3fb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a function to convert seconds to a timedelta\n",
    "def to_timedelta(seconds):\n",
    "    return pd.Timedelta(seconds=seconds)\n",
    "\n",
    "# List of columns to convert\n",
    "columns_to_convert = ['raceTime5K', 'raceTime10K', 'raceTimeHalf', 'raceTimeMarathon']\n",
    "\n",
    "# Apply the formatting function to each race time column\n",
    "for column in columns_to_convert:\n",
    "    combined_racepred[column] = combined_racepred[column].apply(to_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_racepred##.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb5098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_racepred.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cd8b07",
   "metadata": {},
   "source": [
    "### Drop Invaluable columns from Race Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ddcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Columns to drop\n",
    "columns_to_drop = ['deviceId'] ##, 'timestamp']\n",
    "\n",
    "racepred_cleaned = combined_racepred.drop(columns_to_drop, axis=1)\n",
    "\n",
    "## racepred_cleaned.shape (917,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a38ba",
   "metadata": {},
   "source": [
    "### Group by calendarDate and select the MIN race time for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'calendarDate' and find the minimum race times\n",
    "min_race_times = racepred_cleaned.groupby('calendarDate').agg({\n",
    "    'raceTime5K': 'min',\n",
    "    'raceTime10K': 'min',\n",
    "    'raceTimeHalf': 'min',\n",
    "    'raceTimeMarathon': 'min'\n",
    "}).reset_index()\n",
    "\n",
    "# View Results\n",
    "min_race_times\n",
    "\n",
    "## min_race_times['calendarDate'].unique().value_counts()\n",
    "## min_race_times.shape (280,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9db95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2f1ee81",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Race Prediction Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd032435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "min_race_times.to_csv('Processed_Data/RacePredictions_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547fa2b",
   "metadata": {},
   "source": [
    "### Load in Sleep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca630c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in Sleep Data\n",
    "sleep_1_main = pd.read_json('Raw Data/Sleep_Data/2023-10-15_2024-01-23_117832404_sleepData.json')\n",
    "sleep_2_main = pd.read_json('Raw Data/Sleep_Data/2024-01-23_2024-05-02_117832404_sleepData.json')\n",
    "sleep_3_main = pd.read_json('Raw Data/Sleep_Data/2024-05-02_2024-08-10_117832404_sleepData.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "sleep_1 = sleep_1_main.copy()\n",
    "sleep_2 = sleep_2_main.copy()\n",
    "sleep_3 = sleep_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "sleep_1.head()\n",
    "\n",
    "# Assess sleep_1 characteristcs\n",
    "## print(sleep_1.shape)\n",
    "## print(sleep_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f1c94",
   "metadata": {},
   "source": [
    "### Combine 3 Sleep Data Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355b32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_sleep = pd.concat([sleep_1, sleep_2, sleep_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_sleep['calendarDate'] = pd.to_datetime(combined_sleep['calendarDate'])\n",
    "\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'sleepScores' column into separate columns\n",
    "combined_sleep = combined_sleep.join(combined_sleep['sleepScores'].apply(pd.Series))\n",
    "\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1affc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'sleepScores' now that I have extracted information into other columns\n",
    "combined_sleep = combined_sleep.drop('sleepScores',axis=1)\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953bcbb",
   "metadata": {},
   "source": [
    "### Assess Null Values for Sleep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be223aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Dataframe\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sleep start and end timestamps to datetime format\n",
    "combined_sleep['sleepStartTimestampGMT'] = pd.to_datetime(combined_sleep['sleepStartTimestampGMT'])\n",
    "combined_sleep['sleepEndTimestampGMT'] = pd.to_datetime(combined_sleep['sleepEndTimestampGMT'])\n",
    "\n",
    "# Calculate the time difference between sleepStart and sleepEnd\n",
    "combined_sleep['sleepDuration'] = combined_sleep['sleepEndTimestampGMT'] - combined_sleep['sleepStartTimestampGMT']\n",
    "\n",
    "# Create a new column where sleepDuration is in float hours\n",
    "combined_sleep['sleepDurationHours'] = (combined_sleep['sleepDuration'].dt.total_seconds() / 3600).round(1)  # Convert to hours as a float\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e42424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reorder Columns: \n",
    "columns = combined_sleep.columns.to_list()\n",
    "\n",
    "# Get the index of the 'calendarDate' column\n",
    "distance_index = columns.index('calendarDate')\n",
    "\n",
    "# Remove 'sleepDurationHours' and 'sleepDuration' columns from the list\n",
    "columns.remove('sleepDurationHours')\n",
    "columns.remove('sleepDuration')\n",
    "\n",
    "# Insert 'sleepDurationHours' right after 'calendarDate'\n",
    "columns.insert(distance_index + 1, 'sleepDurationHours')\n",
    "\n",
    "# Insert 'sleepDuration' right after 'sleepDurationHours'\n",
    "columns.insert(distance_index + 2, 'sleepDuration')\n",
    "\n",
    "# Reassign the new column order to the DataFrame\n",
    "combined_sleep = combined_sleep[columns]\n",
    "\n",
    "# Drop Columns\n",
    "cols_to_drop = ['sleepStartTimestampGMT','sleepEndTimestampGMT','sleepWindowConfirmationType']\n",
    "combined_sleep = combined_sleep.drop(cols_to_drop,axis=1)\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the last column by position using iloc\n",
    "combined_sleep = combined_sleep.iloc[:, :-1]\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58516a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15323c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Rows in df where 'remSleepSeconds' is null\n",
    "combined_sleep[combined_sleep['remSleepSeconds'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78feae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Data types\n",
    "combined_sleep.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to replace all null values with the column's average value for float64 datatype columns\n",
    "def fill_null_with_mean(df, columns):\n",
    "    for col in columns:\n",
    "        mean_value = df[col].mean()\n",
    "        df[col] = df[col].fillna(mean_value)  # Assign the filled column back to the DataFrame\n",
    "    return df\n",
    "\n",
    "# Get the list of float64 columns\n",
    "float_columns = [col for col in combined_sleep.columns if combined_sleep[col].dtype == 'float64']\n",
    "\n",
    "# Apply the function to replace null values with the mean\n",
    "combined_sleep = fill_null_with_mean(combined_sleep, float_columns)\n",
    "\n",
    "# View the DataFrame\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca122e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rows where 'calendarDate' isna()\n",
    "## combined_sleep[combined_sleep['calendarDate'].isna()]\n",
    "\n",
    "# Manually assign the correct dates to the specific indices where 'calendarDate' is NaT\n",
    "combined_sleep.loc[109, 'calendarDate'] = pd.Timestamp('2024-03-14')\n",
    "combined_sleep.loc[257, 'calendarDate'] = pd.Timestamp('2024-08-09')\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34666f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_sleep.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Make sure 'sleepDurationHours' does not have NaN values before applying\n",
    "combined_sleep['sleepDuration'] = combined_sleep.apply(\n",
    "    lambda row: pd.Timedelta(hours=row['sleepDurationHours']) if pd.isna(row['sleepDuration']) and pd.notna(row['sleepDurationHours']) else row['sleepDuration'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5894f74",
   "metadata": {},
   "source": [
    "#### Re-format 'sleepDuration' so that the field can be converted to a duration dtype in PBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab122aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timedelta to string and remove '0 days '\n",
    "combined_sleep['sleepDurationFormatted'] = combined_sleep['sleepDuration'].apply(lambda x: str(x).split(' ')[-1])\n",
    "\n",
    "# Check the result\n",
    "print(combined_sleep[['sleepDuration', 'sleepDurationFormatted']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e081ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder Columns\n",
    "columns = combined_sleep.columns.to_list()\n",
    "\n",
    "# Get the index of the 'sleepDuration' column\n",
    "sleep_duration_index = columns.index('sleepDuration')\n",
    "\n",
    "# Remove 'sleepDurationFormatted' column from the list\n",
    "columns.remove('sleepDurationFormatted')\n",
    "\n",
    "# Insert 'sleepDurationFormatted' right after 'sleepDuration'\n",
    "columns.insert(sleep_duration_index + 1, 'sleepDurationFormatted')\n",
    "\n",
    "# Reassign the new column order to the DataFrame\n",
    "combined_sleep = combined_sleep[columns]\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f4219",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e30883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round sleepDuration to seconds to remove microseconds and nanoseconds\n",
    "combined_sleep['sleepDuration'] = combined_sleep['sleepDuration'].dt.round('s')  # Use 's' instead of 'S'\n",
    "\n",
    "# If you want to format the duration as 'hh:mm:ss' without nanoseconds\n",
    "combined_sleep['sleepDurationFormatted'] = combined_sleep['sleepDuration'].apply(lambda x: str(x).split(' ')[-1])\n",
    "\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52454a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace NaN values in 'insight' with \"NONE\"\n",
    "combined_sleep['insight'] = combined_sleep['insight'].fillna(\"NONE\")\n",
    "\n",
    "# View value counts for 'feedback' categories\n",
    "## combined_sleep['feedback'].value_counts()\n",
    "\n",
    "# Replace NaN values in 'insight' with \"NONE\"\n",
    "combined_sleep['feedback'] = combined_sleep['feedback'].fillna(\"NONE\")\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9a8a8",
   "metadata": {},
   "source": [
    "### Convert columns LIKE '%Seconds%' to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6a0d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_sleep_cleaned = combined_sleep.copy()\n",
    "\n",
    "# Convert seconds to hours (1 hour = 3600 seconds)\n",
    "def seconds_to_hours(seconds):\n",
    "    return round(seconds / 3600, 1)\n",
    "\n",
    "# Identify columns that contain 'Seconds' in their name\n",
    "columns_to_convert = [col for col in combined_sleep_cleaned.columns if 'Seconds' in col]\n",
    "\n",
    "# Apply the conversion function to these columns\n",
    "for col in columns_to_convert:\n",
    "    combined_sleep_cleaned[col] = combined_sleep_cleaned[col].apply(seconds_to_hours)\n",
    "\n",
    "combined_sleep_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514176f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sleep_cleaned[combined_sleep_cleaned['sleepDuration'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b788970",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86461b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_seconds_to_hours(df):\n",
    "    # Rename columns by replacing 'Seconds' with 'Hours'\n",
    "    df = df.rename(columns={col: col.replace('Seconds', 'Hours') for col in df.columns if 'Seconds' in col})\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "combined_sleep_cleaned = rename_seconds_to_hours(combined_sleep_cleaned)\n",
    "\n",
    "combined_sleep_cleaned.head()\n",
    "\n",
    "# Confirm that there is 1 row per CalendarDate\n",
    "## combined_sleep['calendarDate'].unique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7969152",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sleep_cleaned[combined_sleep_cleaned['sleepDuration'].isna()] # No NaT values anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69beb68",
   "metadata": {},
   "source": [
    "### Drop inisignificant columns for ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bece0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns to drop\n",
    "cols_to_drop_sleep = ['averageRespiration','lowestRespiration','highestRespiration','retro'\n",
    "                      ### Dropping all columns LIKE 'Hours' because the Corresponding Sleep Score should be sufficient\n",
    "                      ,'deepSleepHours','lightSleepHours','remSleepHours','awakeSleepHours','unmeasurableHours','awakeCount'\n",
    "                      ,'restlessMomentCount'\n",
    "                     ]\n",
    "combined_sleep_cleaned_ML = combined_sleep_cleaned.drop(cols_to_drop_sleep,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Floats to Int\n",
    "combined_sleep_cleaned_ML = combined_sleep_cleaned_ML.astype({col: 'int' for col in combined_sleep_cleaned_ML.select_dtypes(include='float').columns})\n",
    "combined_sleep_cleaned_ML.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Floats to Int\n",
    "# combined_sleep_cleaned = combined_sleep_cleaned.astype({col: 'int' for col in combined_sleep_cleaned.select_dtypes(include='float').columns})\n",
    "# combined_sleep_cleaned.head()\n",
    "\n",
    "# Drop Columns\n",
    "cols_to_drop = ['retro','napList']\n",
    "combined_sleep_cleaned = combined_sleep_cleaned.drop(cols_to_drop,axis=1)\n",
    "\n",
    "# View Dataframe\n",
    "combined_sleep_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45370aa8",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Sleep Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38463b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_sleep_cleaned_ML.to_csv('Processed_Data/Sleep_Cleaned_ML.csv', index=False) #For ML Model\n",
    "combined_sleep_cleaned.to_csv('PBI Data/Sleep_Cleaned_PBI.csv', index=False) #For PBI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f44e9",
   "metadata": {},
   "source": [
    "### Load in Summarized Activity Data\n",
    "#### Choosing to ignore this data for initial Model\n",
    "#### See ML_Data_Prep for data transformation on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Summarized Activity Data\n",
    "\n",
    "## activities = pd.read_json('Raw Data/Summarized_Activities/summarizedActivities.json')\n",
    "## activities.head() ### PROBLEM!!! This is a 1x1 data frame\n",
    "### NOTE: Can I manually break the text by a delimiter or something to fix this? YES!!!\n",
    "\n",
    "activities_main = pd.read_json('Raw Data/Summarized_Activities/ahearnzach3@gmail.com_0_summarizedActivities_Cleaned.txt')\n",
    "\n",
    "# Make Copy of Dataframe\n",
    "activities = activities_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "activities ### PROBLEM!!! This is a 1x1 data frame - RESOLVED. Remove first part of json file\n",
    "\n",
    "# Assess activities characteristcs\n",
    "## print(activities.shape)\n",
    "## activities.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d536960",
   "metadata": {},
   "source": [
    "### Load in Training History Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Summarized Activity Data\n",
    "training_hist_1_main = pd.read_json('Raw Data/Training_History/TrainingHistory_20231103_20240211_117832404.json')\n",
    "training_hist_2_main = pd.read_json('Raw Data/Training_History/TrainingHistory_20240211_20240521_117832404.json')\n",
    "training_hist_3_main = pd.read_json('Raw Data/Training_History/TrainingHistory_20240521_20240829_117832404.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "training_hist_1 = training_hist_1_main.copy()\n",
    "training_hist_2 = training_hist_2_main.copy()\n",
    "training_hist_3 = training_hist_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "training_hist_3\n",
    "\n",
    "# Preliminary Analysis\n",
    "\n",
    "### There are multiple records per day as well. Need to factor these changes in.\n",
    "### Maybe it is better to get a daily prediction, and then I can circle back to intra day updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f158d92",
   "metadata": {},
   "source": [
    "### Combine 3 Training History Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42df721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_training_hist = pd.concat([training_hist_1, training_hist_2, training_hist_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_training_hist['calendarDate'] = pd.to_datetime(combined_training_hist['calendarDate'])\n",
    "\n",
    "combined_training_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess combined_training_hist characteristcs\n",
    "print(combined_training_hist.shape)\n",
    "print(combined_training_hist.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba6372",
   "metadata": {},
   "source": [
    "### Group By 'calendarDate' and Select the Last Training Status of Each Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'calendarDate' and get the index of rows with the maximum timestamp\n",
    "idx = combined_training_hist.groupby('calendarDate')['timestamp'].idxmax()\n",
    "\n",
    "# Select the rows with the maximum timestamp for each day\n",
    "combined_training_hist_cleaned = combined_training_hist.loc[idx]\n",
    "\n",
    "# View Results\n",
    "combined_training_hist_cleaned\n",
    "\n",
    "# Confirm that there is only 1 row per 'calendarDate'\n",
    "## combined_training_hist_cleaned['calendarDate'].unique().value_counts()\n",
    "\n",
    "# Assess combined_training_hist_cleaned characteristics\n",
    "## print(combined_training_hist_cleaned.shape) (280,9)\n",
    "## print(combined_training_hist_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd57b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_training_hist_cleaned.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records where 'trainingStatus' is 'NO_STATUS'\n",
    "combined_training_hist_cleaned = combined_training_hist_cleaned[combined_training_hist_cleaned['trainingStatus'] != 'NO_STATUS']\n",
    "combined_training_hist_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_training_hist_cleaned = combined_training_hist_cleaned.drop(['sport'\n",
    "                                                                      ,'subSport'\n",
    "                                                                      ,'deviceId'\n",
    "                                                                      ,'timestamp'\n",
    "                                                                      ,'trainingStatus2FeedbackPhrase'\n",
    "                                                                      ,'userProfilePK']\n",
    "                                                                      , axis=1)\n",
    "combined_training_hist_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_training_hist_cleaned.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3fe00",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Training History Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da237dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_training_hist_cleaned.to_csv('Processed_Data/Training_History_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b71609",
   "metadata": {},
   "source": [
    "### Load in UDS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6717fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in UDS Data\n",
    "uds_1_main = pd.read_json('Raw Data/UDS_Data/UDSFile_2023-10-14_2024-01-22.json')\n",
    "uds_2_main = pd.read_json('Raw Data/UDS_Data/UDSFile_2024-01-22_2024-05-01.json')\n",
    "uds_3_main = pd.read_json('Raw Data/UDS_Data/UDSFile_2024-05-01_2024-08-09.json')\n",
    "\n",
    "# Make Copies of Dataframes\n",
    "uds_1 = uds_1_main.copy()\n",
    "uds_2 = uds_2_main.copy()\n",
    "uds_3 = uds_3_main.copy()\n",
    "\n",
    "# View Imported File\n",
    "uds_1.head()\n",
    "\n",
    "# Assess uds_1 characteristcs\n",
    "## print(uds_1.shape)\n",
    "## print(uds_1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30f6df",
   "metadata": {},
   "source": [
    "### Combine 3 UDS Dataframes into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9fcec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the three DataFrames into one\n",
    "combined_uds = pd.concat([uds_1, uds_2, uds_3], ignore_index=True)\n",
    "\n",
    "# Convert 'calendarDate' to Datetime\n",
    "combined_uds['calendarDate'] = pd.to_datetime(combined_uds['calendarDate'])\n",
    "\n",
    "combined_uds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18486474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess combined_uds characteristcs\n",
    "print(combined_uds.shape)\n",
    "print(combined_uds.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28daf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = combined_uds.isna().sum()\n",
    "\n",
    "# Display the null counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862b932",
   "metadata": {},
   "source": [
    "### Extract data from dictionary type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ec1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922087e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the dictionary-like values in 'allDayStress' and specify suffixes to avoid column overlap\n",
    "combined_uds = combined_uds.join(combined_uds['allDayStress'].apply(pd.Series), rsuffix='_stress')\n",
    "\n",
    "## combined_uds.head()\n",
    "\n",
    "# Drop Columns\n",
    "combined_uds = combined_uds.drop(['allDayStress','calendarDate_stress','userProfilePK_stress'], axis=1)\n",
    "\n",
    "# View Results\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each list of dictionaries in place without exploding the DataFrame\n",
    "expanded_df = pd.json_normalize(combined_uds['aggregatorList'])\n",
    "\n",
    "# Combine the new columns back into the original dataframe without altering row count\n",
    "combined_uds = pd.concat([combined_uds, expanded_df], axis=1)\n",
    "\n",
    "# Drop the original list column if necessary\n",
    "combined_uds = combined_uds.drop(columns=['aggregatorList'])\n",
    "\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66068aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Last 2 columns as the 3rd to last contains all necessary information\n",
    "combined_uds = combined_uds.iloc[:, :-2]\n",
    "\n",
    "# View Results\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename '0' column \n",
    "combined_uds = combined_uds.rename(columns={combined_uds.columns[-1]: 'Total_stress_data'})\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'bodyBattery' column into separate columns\n",
    "combined_uds = combined_uds.join(combined_uds['Total_stress_data'].apply(pd.Series), rsuffix='_stress')\n",
    "\n",
    "# Drop 'bodyBatter' Column\n",
    "combined_uds = combined_uds.drop(['Total_stress_data'], axis=1)\n",
    "\n",
    "combined_uds.head()\n",
    "## combined_uds.shape (259,68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf986f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'bodyBattery' column into separate columns\n",
    "combined_uds = combined_uds.join(combined_uds['bodyBattery'].apply(pd.Series), rsuffix='_battery')\n",
    "\n",
    "# Drop 'bodyBatter' Column\n",
    "combined_uds = combined_uds.drop(['bodyBattery'], axis=1)\n",
    "\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Other Duplicate Columns\n",
    "combined_uds = combined_uds.drop(['userProfilePK_battery','calendarDate_battery'], axis=1)\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bda848",
   "metadata": {},
   "source": [
    "### Don't worry about using this for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize each list of dictionaries in place without exploding the DataFrame\n",
    "# expanded_df = pd.json_normalize(combined_uds['bodyBatteryStatList'])\n",
    "\n",
    "# # Combine the new columns back into the original dataframe without altering row count\n",
    "# combined_uds = pd.concat([combined_uds, expanded_df], axis=1)\n",
    "\n",
    "# # Drop the original list column if necessary\n",
    "# combined_uds = combined_uds.drop(columns=['bodyBatteryStatList'])\n",
    "\n",
    "# combined_uds.head()\n",
    "# ## combined_uds.shape (259,78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ccd44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'respiration' column into separate columns\n",
    "## combined_uds = combined_uds.join(combined_uds['respiration'].apply(pd.Series))\n",
    "\n",
    "## combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'hydration' column into separate columns\n",
    "## combined_uds = combined_uds.join(combined_uds['hydration'].apply(pd.Series))\n",
    "\n",
    "## combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaab14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_uds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e130822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the '0' column\n",
    "## combined_uds = combined_uds.drop(columns=[combined_uds.columns[-1]])\n",
    "# combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ea526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the number of unique values in each column\n",
    "# for col in combined_uds.columns:\n",
    "#     print(f\"{col}: {combined_uds[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f1d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop other insignificant columns\n",
    "combined_uds = combined_uds.drop(['uuid'\n",
    "                                  ,'userProfilePK'\n",
    "                                  ,'wellnessStartTimeGmt'\n",
    "                                  ,'wellnessEndTimeGmt'\n",
    "                                  ,'wellnessStartTimeLocal'\n",
    "                                  ,'wellnessEndTimeLocal'\n",
    "                                  ,'includesWellnessData'\n",
    "                                  ,'includesActivityData'\n",
    "                                  ,'includesCalorieConsumedData'\n",
    "                                  ,'includesSingleMeasurement'\n",
    "                                  ,'includesContinuousMeasurement'\n",
    "                                  ,'includesAllDayPulseOx'\n",
    "                                  ,'includesSleepPulseOx'\n",
    "                                  ,'source'\n",
    "                                  ,'userFloorsAscendedGoal'\n",
    "                                 ], axis=1)\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db15792",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uds = combined_uds.drop(['durationInMilliseconds'\n",
    "                                  ,'wellnessKilocalories'\n",
    "                                  ,'remainingKilocalories' # Same Values as 'wellnessKilocalories'\n",
    "                                  ,'wellnessTotalKilocalories' # Same Values as 'wellnessKilocalories'\n",
    "                                  ,'wellnessActiveKilocalories'\n",
    "                                  ,'dailyStepGoal'\n",
    "                                  ,'wellnessDistanceMeters'\n",
    "                                  ,'userIntensityMinutesGoal'\n",
    "                                  ,'minAvgHeartRate'\n",
    "                                  ,'maxAvgHeartRate'\n",
    "                                  ,'version'\n",
    "                                  ,'restingCaloriesFromActivity'\n",
    "                                  ,'restingHeartRateTimestamp'\n",
    "                                 # ,'hydration'\n",
    "                                  ,'dailyTotalFromEpochData'\n",
    "                                  ,'type'\n",
    "                                  ,'uncategorizedDuration'\n",
    "                                  ,'totalDuration'\n",
    "                                  ,'lowDuration'\n",
    "                                  ,'bodyBatteryVersion'\n",
    "                                 ],axis=1)\n",
    "                                  \n",
    "                                  \n",
    "combined_uds.head()   \n",
    "\n",
    "## combined_uds.shape (259, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'respiration' column into separate columns\n",
    "respiration = combined_uds.join(combined_uds['respiration'].apply(pd.Series), rsuffix='_respiration')\n",
    "respiration.head()\n",
    "\n",
    "### I don't see much value in these columns. Going to drop 'respiration' from df without using any of the dictionary columns\n",
    "\n",
    "# Drop 'respiration' Column\n",
    "combined_uds = combined_uds.drop(['respiration'], axis=1)\n",
    "\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'hydration' column into separate columns\n",
    "hydration = combined_uds.join(combined_uds['hydration'].apply(pd.Series), rsuffix='_hydration')\n",
    "hydration.head()\n",
    "\n",
    "### I don't see much value in these columns. Going to drop 'hydration' from df without using any of the dictionary columns\n",
    "\n",
    "# Drop 'hydration' Column\n",
    "combined_uds = combined_uds.drop(['hydration'], axis=1)\n",
    "\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe500186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary like values in 'bodyBatteryFeedback' column into separate columns\n",
    "bodyBatteryFeedback = combined_uds.join(combined_uds['bodyBatteryFeedback'].apply(pd.Series), rsuffix='_bodyBatteryFeedback')\n",
    "bodyBatteryFeedback\n",
    "\n",
    "### I don't see much value in these columns for intial model.\n",
    "### Going to drop 'bodyBatteryFeedback' from df without using any of the dictionary columns\n",
    "### May be more value in 'bodyBatteryFeedback' for later models\n",
    "\n",
    "# Drop 'bodyBatteryFeedback' Column\n",
    "combined_uds = combined_uds.drop(['bodyBatteryFeedback'], axis=1)\n",
    "\n",
    "# Drop 'bodyBatteryStatList' Column\n",
    "combined_uds = combined_uds.drop(['bodyBatteryStatList'], axis=1)\n",
    "combined_uds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb985863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0.0 in the 'isVigorousDay' column\n",
    "combined_uds['isVigorousDay'] = combined_uds['isVigorousDay'].fillna(0.0)\n",
    "\n",
    "# Verify the changes\n",
    "combined_uds['isVigorousDay'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Last Columns of Dataframe\n",
    "combined_uds = combined_uds.iloc[:, :-1]\n",
    "combined_uds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf75a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dfce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uds[combined_uds['activityDuration'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1972588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in each column with the column's mean\n",
    "combined_uds = combined_uds.fillna(combined_uds.mean(numeric_only=True))\n",
    "combined_uds\n",
    "\n",
    "# Verify that the missing values have been filled\n",
    "#combined_uds.isna().sum() # This should show 0 for columns that had missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert all 'float' type columns to 'int'\n",
    "combined_uds = combined_uds.astype({col: 'int' for col in combined_uds.select_dtypes(include='float').columns})\n",
    "combined_uds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f5544",
   "metadata": {},
   "source": [
    "### Save Pre-Processed UDS Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd7494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_uds.to_csv('Processed_Data/UDS_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4611da08",
   "metadata": {},
   "source": [
    "### Load Running Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "running_main = pd.read_csv(\"Workout_Data_20240804.csv\")\n",
    "## running_main.head()\n",
    "\n",
    "# Create a copy of the original dataset\n",
    "running = running_main.copy()\n",
    "running.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess Dimensionality and Columns Names\n",
    "print(running.shape)\n",
    "print(running.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb81ce",
   "metadata": {},
   "source": [
    "### 1. Running Data Cleaning and Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53958df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all NULL columns\n",
    "print(running.shape)\n",
    "print(running.dropna(axis = 1, how='all').shape) #Compare shape after dropping null columns\n",
    "\n",
    "running_cleaned = running.dropna(axis = 1, how='all')\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ddf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add _ into Column Headers\n",
    "running_cleaned.columns = running_cleaned.columns.str.replace(' ', '_')\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove special characters from column names\n",
    "running_cleaned.columns = running_cleaned.columns.str.replace(r'[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "## df_cleaned[['Normalized_Power_NP']]\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a17a0",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Distant Group Column to group runs into mileage buckets\n",
    "bins = [0, 3, 5, 7, 10, 13, float('inf')]\n",
    "labels = ['0-3 miles', '3-5 miles', '5-7 miles', '7-10 miles', '10-13 miles', '13+ miles']\n",
    "\n",
    "# Use Distance Value to assign Distance Group\n",
    "\n",
    "## running_cleaned.loc[:, 'Distance Group'] = pd.cut(df_cleaned['Distance'], bins=bins, labels=labels, right=False) -> This was causing an error message\n",
    "running_cleaned = running_cleaned.assign(Distance_Group=pd.cut(running_cleaned['Distance'], bins=bins, labels=labels, right=False))\n",
    "\n",
    "# View Results\n",
    "print(running_cleaned[['Distance', 'Distance_Group']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6495127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Favorite Column as I have not been updating that field\n",
    "running_cleaned = running_cleaned.drop('Favorite', axis = 1) #-> UNCOMMENT BEFORE RUNNING THIS CELL!!!\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8550ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'Distance_Group' directly after 'Distance'\n",
    "columns = running_cleaned.columns.to_list()\n",
    "\n",
    "# Get the index of the 'Distance' column\n",
    "distance_index = columns.index('Distance')\n",
    "\n",
    "# Insert 'Distance Group' right after 'Distance'\n",
    "columns.insert(distance_index + 1, columns.pop(columns.index('Distance_Group')))\n",
    "\n",
    "# Update df_cleaned with new Column Order\n",
    "running_cleaned = running_cleaned[columns]\n",
    "\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' to datetime and set the time to 00:00:00\n",
    "running_cleaned['Date'] = pd.to_datetime(running_cleaned['Date']).dt.normalize()\n",
    "\n",
    "# Rename 'Date' to 'calendarDate'\n",
    "running_cleaned.rename(columns={'Date': 'calendarDate'}, inplace=True)\n",
    "\n",
    "print(running_cleaned.dtypes.head())\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Week of Year Field\n",
    "## running_cleaned['Week_of_Year'] = running_cleaned['Date'].dt.isocalendar().week\n",
    "\n",
    "# Create Month Field\n",
    "## running_cleaned['Month'] = running_cleaned['Date'].dt.month\n",
    "\n",
    "# Create Year Field\n",
    "## running_cleaned['Year'] = running_cleaned['Date'].dt.year\n",
    "\n",
    "# View New Fields and Data Types\n",
    "## print(running_cleaned[['Date','Week_of_Year','Month','Year']].dtypes)\n",
    "## running_cleaned[['Date','Week_of_Year','Month','Year']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e89b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_cleaned.isna().sum() ### No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb377c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive a count of unique values for each column in running_cleaned ### USE THIS BEFORE JOINING ALL DATAFRAMES TOGETHER\n",
    "for col in running_cleaned.columns:\n",
    "    print(f\"{col}: {running_cleaned[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c985c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_cleaned = running_cleaned.drop(['Decompression','Training_Stress_Score'], axis=1)\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c532f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f10bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '--' with 0 in 'Total_Ascent' and 'Total_Descent' columns\n",
    "running_cleaned[['Total_Ascent', 'Total_Descent']] = running_cleaned[['Total_Ascent', 'Total_Descent']].replace('--', 0)\n",
    "\n",
    "# Verify the changes\n",
    "running_cleaned[['Total_Ascent', 'Total_Descent']].head()\n",
    "## running_cleaned[running_cleaned['Total_Ascent']== '--']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the averages for the 'Max_Elevation' and 'Min_Elevation' where Title == 'Charlotte Running'\n",
    "charlotte_running_mask = running_cleaned['Title'] == 'Charlotte Running'\n",
    "avg_max_elevation = running_cleaned.loc[charlotte_running_mask, 'Max_Elevation'].replace('--', pd.NA).astype(float).mean()\n",
    "avg_min_elevation = running_cleaned.loc[charlotte_running_mask, 'Min_Elevation'].replace('--', pd.NA).astype(float).mean()\n",
    "\n",
    "# Replace '--' with the calculated averages and round to nearest integer\n",
    "running_cleaned['Max_Elevation'] = running_cleaned['Max_Elevation'].replace('--', avg_max_elevation).astype(float).round().astype(int)\n",
    "running_cleaned['Min_Elevation'] = running_cleaned['Min_Elevation'].replace('--', avg_min_elevation).astype(float).round().astype(int)\n",
    "\n",
    "# Verify the changes\n",
    "running_cleaned[['Max_Elevation', 'Min_Elevation']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa384f",
   "metadata": {},
   "source": [
    "### For Training Status and Race Predictor Purposes I can remove a fair amount of columns\n",
    "#### Many of these columns will be running specific metrics that are either very similar throughout the column or columns that I am using domain knowledge to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cd824",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Avg_Run_Cadence','Max_Run_Cadence','Avg_Stride_Length','Avg_Vertical_Ratio','Avg_Vertical_Oscillation'\n",
    "                ,'Avg_Ground_Contact_Time','Avg_GAP','Normalized_Power_NP','Avg_Power','Max_Power','Best_Lap_Time'\n",
    "                ,'Number_of_Laps','Moving_Time','Elapsed_Time','Title'\n",
    "               ]\n",
    "running_cleaned = running_cleaned.drop(cols_to_drop, axis=1)\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check run count per day\n",
    "running_cleaned['calendarDate'].value_counts()\n",
    "\n",
    "# Keep only the record with the longest run (max Distance) for each calendarDate\n",
    "running_cleaned = running_cleaned.loc[running_cleaned.groupby('calendarDate')['Distance'].idxmax()]\n",
    "running_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2c7ac",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Running Data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "running_cleaned.to_csv('Processed_Data/Running_Cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
